============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.mcq.MCQPromptStrategy
  name: mcq
  few_shot: true
  output_format: json
  answer_first: false
  contrarian: false
dataset:
  _target_: cotlab.datasets.loaders.MedBulletsDataset
  name: medbullets
  split: op5_test
experiment:
  _target_: cotlab.experiments.AttentionAnalysisExperiment
  name: attention_analysis
  description: Analyze attention patterns at critical layers
  all_layers: true
  target_layers: null
  layer_stride: 1
  force_eager_reload: false
  num_samples: null
  last_k_tokens: 16
  max_input_tokens: 1024
  analyze_generated_tokens: false
  generated_max_new_tokens: 16
  generated_do_sample: false
  generated_temperature: 0.7
  generated_top_p: 0.9
  question: Patient presents with chest pain, sweating, and shortness of breath. What
    is the diagnosis?
  batch_size: 16
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /root/CoTLab/CoTLab/outputs/2026-02-26/16-12-22_attention_analysis_medgemma_27b_text_it_mcq_json_medbullets/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:06,  1.48it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.45it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.43it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:04,  1.43it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.43it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.43it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:04<00:02,  1.42it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.42it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.42it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.44it/s]
  Resolved device: cuda:0
Creating prompt strategy: mcq
Loading dataset: medbullets
medbullets/op5_test-00000-of-00001.parqu(…):   0%|                     | 0.00/606k [00:00<?, ?B/s]medbullets/op5_test-00000-of-00001.parqu(…):   0%|                     | 0.00/606k [00:00<?, ?B/s]medbullets/op5_test-00000-of-00001.parqu(…):   0%|                     | 0.00/606k [00:00<?, ?B/s]medbullets/op5_test-00000-of-00001.parqu(…):   0%|                     | 0.00/606k [00:00<?, ?B/s]medbullets/op5_test-00000-of-00001.parqu(…):  22%|██▊          | 131k/606k [00:00<00:00, 1.24MB/s]medbullets/op5_test-00000-of-00001.parqu(…): 100%|██████████████| 606k/606k [00:01<00:00, 594kB/s]
Creating experiment: attention_analysis
============================================================
Running experiment: attention_analysis
============================================================
Model: google/medgemma-27b-text-it
Attention heads: 32
All layers enabled: True
Layer stride: 1
Resolved layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
Max input tokens: 1024
Batch size: 16
Analyze generated tokens: False
Current attention implementation: sdpa
Switching attention implementation to 'eager' in-place...

Analyzing attention on 308 samples (batch_size=16)...
Processing batches:   0%|                                                  | 0/20 [00:00<?, ?it/s]Processing batches:   5%|██                                        | 1/20 [00:07<02:13,  7.04s/it]Processing batches:  10%|████▏                                     | 2/20 [00:13<02:03,  6.88s/it]Processing batches:  15%|██████▎                                   | 3/20 [00:20<01:57,  6.92s/it]Processing batches:  20%|████████▍                                 | 4/20 [00:27<01:50,  6.92s/it]Processing batches:  25%|██████████▌                               | 5/20 [00:34<01:43,  6.91s/it]Processing batches:  30%|████████████▌                             | 6/20 [00:41<01:37,  6.93s/it]Processing batches:  35%|██████████████▋                           | 7/20 [00:48<01:29,  6.91s/it]Processing batches:  40%|████████████████▊                         | 8/20 [00:55<01:22,  6.91s/it]Processing batches:  45%|██████████████████▉                       | 9/20 [01:02<01:16,  6.93s/it]Processing batches:  50%|████████████████████▌                    | 10/20 [01:09<01:09,  6.99s/it]Processing batches:  55%|██████████████████████▌                  | 11/20 [01:16<01:03,  7.03s/it]Processing batches:  60%|████████████████████████▌                | 12/20 [01:23<00:55,  6.97s/it]Processing batches:  65%|██████████████████████████▋              | 13/20 [01:30<00:48,  6.94s/it]Processing batches:  70%|████████████████████████████▋            | 14/20 [01:37<00:41,  6.94s/it]Processing batches:  75%|██████████████████████████████▊          | 15/20 [01:44<00:34,  6.93s/it]Processing batches:  80%|████████████████████████████████▊        | 16/20 [01:51<00:27,  6.95s/it]Processing batches:  85%|██████████████████████████████████▊      | 17/20 [01:58<00:20,  6.96s/it]Processing batches:  90%|████████████████████████████████████▉    | 18/20 [02:04<00:13,  6.91s/it]Processing batches:  95%|██████████████████████████████████████▉  | 19/20 [02:11<00:06,  6.93s/it]Processing batches: 100%|█████████████████████████████████████████| 20/20 [02:13<00:00,  5.36s/it]Processing batches: 100%|█████████████████████████████████████████| 20/20 [02:13<00:00,  6.68s/it]

======================================================================
ATTENTION ANALYSIS: Aggregated Statistics Across Samples
======================================================================
Layer    | LastTok μ  | AllTok μ   | Last16 μ   | AllTok σ   | Top Tokens
----------------------------------------------------------------------------------------------------------
L0       | 3.7474     | 3.4319     | 4.1965     | 0.0397     | '.', ',', ' of'
L1       | 1.8026     | 1.4877     | 1.9278     | 0.0529     | ' ', '<bos>', '.'
L2       | 1.9369     | 1.4710     | 2.1666     | 0.0789     | '.', ' "', ' '
L3       | 0.6280     | 0.3118     | 0.6084     | 0.0431     | '.', ' answer', ' '
L4       | 1.1518     | 0.9661     | 1.2262     | 0.0216     | ' ', '<bos>', '.'
L5       | 1.5155     | 1.1065     | 1.3915     | 0.0205     | ' ', '<bos>', '’'
L6       | 2.1527     | 1.5723     | 2.1987     | 0.0801     | ' ', '<bos>', ' ...)'
L7       | 2.1497     | 1.5806     | 2.0884     | 0.0474     | ' answer', ' correct', '.'
L8       | 2.3444     | 1.7621     | 2.1964     | 0.0512     | '.', ' answer', ' '
L9       | 1.9959     | 1.6022     | 2.1249     | 0.0472     | '.', ' answer', '
'
L10      | 2.3227     | 1.6638     | 2.4072     | 0.0431     | ''', '<bos>', ' '
L11      | 1.6405     | 1.4879     | 1.9240     | 0.0289     | ''', '<bos>', ' '
L12      | 1.8713     | 1.2654     | 1.8954     | 0.0576     | ' ', '<bos>', '''
L13      | 2.3548     | 1.5407     | 2.1810     | 0.0538     | ' ', '<bos>', '''
L14      | 2.2784     | 1.6160     | 2.0898     | 0.0531     | '.', ':', '
'
L15      | 2.3285     | 1.5072     | 2.0390     | 0.0394     | '<bos>', ' ', '''
L16      | 1.9864     | 1.6261     | 1.9754     | 0.0310     | ' ', '<bos>', '''
L17      | 2.6548     | 2.0500     | 2.5593     | 0.0312     | '<bos>', ''', ' '
L18      | 3.2088     | 2.5629     | 3.1867     | 0.0396     | ',', '.', ' answer'
L19      | 2.6512     | 1.9763     | 2.5160     | 0.0348     | '<bos>', ' ', '''
L20      | 2.8807     | 2.1190     | 2.7142     | 0.0320     | '.', ' answer', ' D'
L21      | 2.9083     | 2.3287     | 2.7563     | 0.0377     | '.', ' answer', '```'
L22      | 3.1128     | 2.3128     | 2.7916     | 0.0342     | '.', ' answer', ' of'
L23      | 3.1061     | 2.5749     | 2.9542     | 0.0405     | '<bos>', '```', ' '
L24      | 3.4285     | 2.6393     | 3.2355     | 0.0304     | '.', '"}', ' answer'
L25      | 3.4806     | 2.7498     | 3.1642     | 0.0264     | '
', '.', ' answer'
L26      | 3.5181     | 2.7433     | 3.2864     | 0.0317     | '.', '```', '<bos>'
L27      | 3.3122     | 2.7270     | 3.1308     | 0.0418     | '.', ':', ' ...)'
L28      | 3.3997     | 2.6751     | 3.2418     | 0.0344     | ' ', '<bos>', '''
L29      | 2.7348     | 2.2179     | 2.7683     | 0.0329     | '<bos>', ' ', '```'
L30      | 2.8721     | 2.2990     | 2.8255     | 0.0332     | ' ', '<bos>', '",'
L31      | 2.9118     | 2.0588     | 2.6058     | 0.0366     | ' ', '<bos>', '
'
L32      | 2.6410     | 1.8680     | 2.4257     | 0.0383     | ' ', '<bos>', ' format'
L33      | 2.8281     | 1.8912     | 2.4818     | 0.0410     | ' ', '<bos>', 'X'
L34      | 2.3789     | 1.6503     | 2.1106     | 0.0400     | ' ', '<bos>', ' answer'
L35      | 2.9563     | 2.0580     | 2.6611     | 0.0362     | ' ', '<bos>', '```'
L36      | 2.7094     | 1.9965     | 2.4574     | 0.0317     | ' ', '<bos>', '.'
L37      | 2.8281     | 1.9093     | 2.6344     | 0.0318     | ' ', '<bos>', 'answer'
L38      | 2.3988     | 1.4547     | 1.9882     | 0.0387     | ' ', '.', '<bos>'
L39      | 2.1727     | 1.3288     | 1.8517     | 0.0380     | ' ', '<bos>', ' "'
L40      | 1.4728     | 0.6997     | 1.3071     | 0.0488     | ' ', '<bos>', ' answer'
L41      | 1.9301     | 0.9508     | 1.6102     | 0.0320     | ' ', '<bos>', '##'
L42      | 2.3703     | 1.3493     | 1.9794     | 0.0411     | ' ', '<bos>', 'json'
L43      | 1.5267     | 0.9713     | 1.5566     | 0.0472     | ' ', '<bos>', ' correct'
L44      | 1.5791     | 1.0025     | 1.4938     | 0.0461     | ' ', '<bos>', '.'
L45      | 1.6705     | 0.9202     | 1.4936     | 0.0459     | ' ', '<bos>', ' "'
L46      | 1.5469     | 0.9976     | 1.5856     | 0.0447     | ' ', '<bos>', '.'
L47      | 2.3541     | 1.3653     | 2.3651     | 0.0360     | ' ', '<bos>', '.'
L48      | 1.3118     | 0.6317     | 1.3572     | 0.0537     | ' ', '<bos>', ' correct'
L49      | 1.2862     | 0.6244     | 1.2312     | 0.0520     | ' ', '<bos>', 'g'
L50      | 1.1576     | 0.6385     | 1.2107     | 0.0504     | ' ', '<bos>', '.'
L51      | 1.4822     | 0.8443     | 1.4727     | 0.0492     | ' ', '<bos>', 'X'
L52      | 1.7504     | 1.1028     | 1.6078     | 0.0430     | ' ', '<bos>', '.'
L53      | 2.3484     | 1.3343     | 2.0998     | 0.0371     | ' ', '<bos>', '
'
L54      | 2.2027     | 1.6331     | 2.0895     | 0.0380     | ' ', '<bos>', '```'
L55      | 2.3869     | 1.4998     | 2.1459     | 0.0407     | ' ', '<bos>', '''
L56      | 1.8932     | 1.3401     | 1.7801     | 0.0399     | ' ', '<bos>', '.'
L57      | 1.3202     | 0.7474     | 1.2615     | 0.0498     | ' ', '<bos>', '.'
L58      | 2.7010     | 1.8025     | 2.2946     | 0.0399     | '.', '<bos>', ' '
L59      | 2.4313     | 1.3293     | 2.0414     | 0.0441     | '<bos>', ' ', '

'
L60      | 1.8677     | 1.2573     | 1.7419     | 0.0419     | '<bos>', ' ', '.'
L61      | 1.8989     | 1.3126     | 1.7115     | 0.0338     | '<bos>', '.', ' '
----------------------------------------------------------------------------------------------------------

Overall mean entropy (all tokens): 1.6217
Overall mean entropy (last token): 2.2869
Overall mean entropy (last 16 tokens): 2.1681
Most focused layer (all tokens): L3 (entropy: 0.3118)

Results saved to: /root/CoTLab/CoTLab/outputs/2026-02-26/16-12-22_attention_analysis_medgemma_27b_text_it_mcq_json_medbullets/results.json

Metrics:
  num_samples_analyzed: 308
  num_layers_analyzed: 62
  num_heads: 32
  overall_mean_entropy: 1.6217023176920258
  overall_mean_entropy_all_tokens: 1.6217023176920258
  overall_mean_entropy_last_token: 2.286939119444605
  overall_mean_entropy_last_k_tokens: 2.1680888113142167
  last_k_tokens: 16
  most_focused_layer: 3
  most_focused_entropy: 0.3118277209380095
  most_focused_layer_all_tokens: 3
  most_focused_entropy_all_tokens: 0.3118277209380095
  most_focused_layer_last_token: 3
  most_focused_entropy_last_token: 0.6280451126443671
  analyze_generated_tokens: False

Experiment documentation updated: /root/CoTLab/CoTLab/outputs/2026-02-26/16-12-22_attention_analysis_medgemma_27b_text_it_mcq_json_medbullets/EXPERIMENT.md

Experiment complete.
