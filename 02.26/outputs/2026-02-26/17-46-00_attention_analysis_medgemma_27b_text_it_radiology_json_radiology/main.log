============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.RadiologyPromptStrategy
  name: radiology
  contrarian: false
  few_shot: true
  answer_first: false
  output_format: json
dataset:
  _target_: cotlab.datasets.RadiologyDataset
  name: radiology
  path: data/radiology.json
experiment:
  _target_: cotlab.experiments.AttentionAnalysisExperiment
  name: attention_analysis
  description: Analyze attention patterns at critical layers
  all_layers: true
  target_layers: null
  layer_stride: 1
  force_eager_reload: false
  num_samples: null
  last_k_tokens: 16
  max_input_tokens: 1024
  analyze_generated_tokens: false
  generated_max_new_tokens: 16
  generated_do_sample: false
  generated_temperature: 0.7
  generated_top_p: 0.9
  question: Patient presents with chest pain, sweating, and shortness of breath. What
    is the diagnosis?
  batch_size: 4
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /home/ubuntu/CoTLab/outputs/2026-02-26/17-46-00_attention_analysis_medgemma_27b_text_it_radiology_json_radiology/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:07,  1.42it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.40it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.41it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:04,  1.40it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.42it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.43it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:04<00:02,  1.41it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.41it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.42it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.47it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.43it/s]
  Resolved device: cuda:0
Creating prompt strategy: radiology
Loading dataset: radiology
Creating experiment: attention_analysis
============================================================
Running experiment: attention_analysis
============================================================
Model: google/medgemma-27b-text-it
Attention heads: 32
All layers enabled: True
Layer stride: 1
Resolved layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
Max input tokens: 1024
Batch size: 4
Analyze generated tokens: False
Current attention implementation: sdpa
Switching attention implementation to 'eager' in-place...

Analyzing attention on 100 samples (batch_size=4)...
Processing batches:   0%|                                                  | 0/25 [00:00<?, ?it/s]Processing batches:   4%|█▋                                        | 1/25 [00:02<00:51,  2.13s/it]Processing batches:   8%|███▎                                      | 2/25 [00:03<00:45,  1.96s/it]Processing batches:  12%|█████                                     | 3/25 [00:05<00:41,  1.87s/it]Processing batches:  16%|██████▋                                   | 4/25 [00:07<00:38,  1.82s/it]Processing batches:  20%|████████▍                                 | 5/25 [00:09<00:36,  1.80s/it]Processing batches:  24%|██████████                                | 6/25 [00:11<00:34,  1.79s/it]Processing batches:  28%|███████████▊                              | 7/25 [00:12<00:32,  1.79s/it]Processing batches:  32%|█████████████▍                            | 8/25 [00:14<00:30,  1.78s/it]Processing batches:  36%|███████████████                           | 9/25 [00:16<00:28,  1.77s/it]Processing batches:  40%|████████████████▍                        | 10/25 [00:18<00:26,  1.77s/it]Processing batches:  44%|██████████████████                       | 11/25 [00:19<00:24,  1.77s/it]Processing batches:  48%|███████████████████▋                     | 12/25 [00:21<00:23,  1.78s/it]Processing batches:  52%|█████████████████████▎                   | 13/25 [00:23<00:21,  1.81s/it]Processing batches:  56%|██████████████████████▉                  | 14/25 [00:25<00:19,  1.82s/it]Processing batches:  60%|████████████████████████▌                | 15/25 [00:27<00:17,  1.80s/it]Processing batches:  64%|██████████████████████████▏              | 16/25 [00:28<00:16,  1.80s/it]Processing batches:  68%|███████████████████████████▉             | 17/25 [00:30<00:14,  1.78s/it]Processing batches:  72%|█████████████████████████████▌           | 18/25 [00:32<00:12,  1.81s/it]Processing batches:  76%|███████████████████████████████▏         | 19/25 [00:34<00:10,  1.79s/it]Processing batches:  80%|████████████████████████████████▊        | 20/25 [00:36<00:08,  1.78s/it]Processing batches:  84%|██████████████████████████████████▍      | 21/25 [00:37<00:07,  1.81s/it]Processing batches:  88%|████████████████████████████████████     | 22/25 [00:39<00:05,  1.86s/it]Processing batches:  92%|█████████████████████████████████████▋   | 23/25 [00:41<00:03,  1.83s/it]Processing batches:  96%|███████████████████████████████████████▎ | 24/25 [00:43<00:01,  1.81s/it]Processing batches: 100%|█████████████████████████████████████████| 25/25 [00:45<00:00,  1.79s/it]Processing batches: 100%|█████████████████████████████████████████| 25/25 [00:45<00:00,  1.81s/it]

======================================================================
ATTENTION ANALYSIS: Aggregated Statistics Across Samples
======================================================================
Layer    | LastTok μ  | AllTok μ   | Last16 μ   | AllTok σ   | Top Tokens
----------------------------------------------------------------------------------------------------------
L0       | 3.6816     | 3.1207     | 3.6554     | 0.0619     | ':', 'Response', '
'
L1       | 1.1428     | 1.2240     | 1.4625     | 0.0317     | ' ', '<bos>'
L2       | 1.4011     | 1.1544     | 1.4966     | 0.0352     | ' ', '<bos>'
L3       | 0.1978     | 0.1611     | 0.2989     | 0.0142     | ' ', '<bos>'
L4       | 0.8245     | 0.8685     | 0.9908     | 0.0157     | ' ', '<bos>'
L5       | 1.1950     | 0.9739     | 1.1709     | 0.0200     | ' ', '<bos>'
L6       | 1.3889     | 1.2376     | 1.7776     | 0.0381     | ' ', '<bos>', 'Response'
L7       | 1.5315     | 1.3469     | 1.5960     | 0.0334     | ' ', '<bos>'
L8       | 1.6099     | 1.5138     | 1.8813     | 0.0348     | ' ', '<bos>', ':'
L9       | 1.6580     | 1.4186     | 1.9055     | 0.0363     | '
', ':', ' '
L10      | 2.0311     | 1.5016     | 2.1792     | 0.0374     | ' ', '<bos>', '"""'
L11      | 2.1805     | 1.3389     | 1.8945     | 0.0234     | ' ', '<bos>'
L12      | 1.6075     | 0.9840     | 1.5755     | 0.0449     | ' ', '<bos>', 'Response'
L13      | 1.8751     | 1.2561     | 1.9157     | 0.0468     | ' ', '<bos>', 'Response'
L14      | 1.6466     | 1.4302     | 1.8505     | 0.0434     | ' ', '<bos>', '
'
L15      | 2.0900     | 1.3790     | 1.9767     | 0.0312     | ' ', '
', '<bos>'
L16      | 1.7386     | 1.4547     | 1.8186     | 0.0309     | ' ', '<bos>'
L17      | 2.7089     | 1.9845     | 2.5800     | 0.0299     | ' ', '<bos>'
L18      | 3.0681     | 2.4300     | 3.0680     | 0.0428     | ':', 'Response', '
'
L19      | 2.2678     | 1.8721     | 2.3548     | 0.0310     | ' ', '<bos>', ' json'
L20      | 2.6000     | 2.1028     | 2.5985     | 0.0241     | '
', ':', '

'
L21      | 2.5231     | 2.2450     | 2.6854     | 0.0360     | ':', '
', 'Response'
L22      | 2.8705     | 2.2627     | 2.7744     | 0.0263     | '
', ':', '

'
L23      | 3.0929     | 2.5797     | 3.1952     | 0.0365     | ' ', '<bos>', 'Only'
L24      | 3.0012     | 2.6102     | 3.0724     | 0.0314     | '
', ':', 'Follow'
L25      | 2.8891     | 2.7811     | 3.1873     | 0.0202     | '
', ':', 'Radi'
L26      | 3.0330     | 2.7730     | 3.2292     | 0.0261     | '
', ':', 'Response'
L27      | 3.0283     | 2.7688     | 3.1918     | 0.0374     | '
', 'Response', '

'
L28      | 2.9238     | 2.6841     | 3.1287     | 0.0296     | 'Response', '<bos>', ' '
L29      | 2.6482     | 2.3901     | 2.8386     | 0.0309     | '<bos>', ' ', '"""'
L30      | 2.8726     | 2.3532     | 2.7936     | 0.0272     | 'Response', '<bos>', ' '
L31      | 2.4955     | 2.1095     | 2.4446     | 0.0295     | '<bos>', 'Response', ' '
L32      | 2.3421     | 1.9651     | 2.3055     | 0.0251     | '<bos>', ' ', 'Response'
L33      | 2.2612     | 1.9042     | 2.3593     | 0.0341     | 'Response', '<bos>', ' '
L34      | 1.9068     | 1.6990     | 1.9633     | 0.0308     | '
', '<bos>', ' '
L35      | 3.0669     | 2.1375     | 2.7197     | 0.0333     | ' ', '<bos>', 'Response'
L36      | 2.1543     | 2.0489     | 2.3053     | 0.0241     | ' ', '<bos>', 'Response'
L37      | 2.5405     | 1.9454     | 2.3119     | 0.0255     | ' ', '<bos>', ':'
L38      | 1.6822     | 1.4903     | 1.7786     | 0.0301     | '<bos>', ' ', 'Response'
L39      | 1.4828     | 1.3648     | 1.5540     | 0.0309     | ' ', '<bos>', '
'
L40      | 1.1277     | 0.7270     | 1.0216     | 0.0296     | ' ', '<bos>', 'Response'
L41      | 1.7478     | 1.0632     | 1.4683     | 0.0223     | ' ', '<bos>', ' the'
L42      | 1.6974     | 1.3716     | 1.6057     | 0.0309     | ' ', '<bos>', 'Response'
L43      | 1.0665     | 0.8981     | 1.1776     | 0.0336     | ' ', '<bos>', 'Response'
L44      | 1.2701     | 0.9915     | 1.2953     | 0.0328     | ' ', '<bos>', '
'
L45      | 1.1087     | 0.9143     | 1.1762     | 0.0309     | ' ', '<bos>', 'Follow'
L46      | 1.1224     | 1.0482     | 1.3297     | 0.0303     | ' ', '<bos>', 'Response'
L47      | 2.2063     | 1.6926     | 1.9520     | 0.0242     | ' ', '<bos>', '{'
L48      | 0.9633     | 0.6384     | 1.0086     | 0.0334     | ' ', '<bos>', 'Response'
L49      | 0.9633     | 0.5777     | 0.9499     | 0.0353     | ' ', '<bos>', '

'
L50      | 0.9441     | 0.6036     | 0.9615     | 0.0337     | ' ', '<bos>', 'Response'
L51      | 1.2350     | 0.8159     | 1.1178     | 0.0345     | ' ', '<bos>', 'Response'
L52      | 1.4612     | 1.1064     | 1.4100     | 0.0298     | ' ', '<bos>', 'Response'
L53      | 2.2528     | 1.5666     | 1.8015     | 0.0338     | ' ', '<bos>', 'Follow'
L54      | 2.0130     | 1.6335     | 1.8341     | 0.0363     | ' ', '<bos>', '
'
L55      | 2.0726     | 1.5857     | 1.9490     | 0.0371     | ' ', '<bos>', ' the'
L56      | 1.5740     | 1.3454     | 1.5812     | 0.0359     | ' ', '<bos>', '
'
L57      | 1.1919     | 0.7328     | 1.1434     | 0.0334     | ' ', '<bos>', 'Response'
L58      | 2.0718     | 1.8898     | 2.1789     | 0.0454     | ' ', '<bos>', 'Response'
L59      | 2.7634     | 1.6289     | 2.1551     | 0.0364     | '
', '<bos>', '

'
L60      | 1.5807     | 1.2702     | 1.5703     | 0.0280     | ' ', '<bos>', '
'
L61      | 1.5084     | 1.3018     | 1.5520     | 0.0292     | '
', '<bos>', ' '
----------------------------------------------------------------------------------------------------------

Overall mean entropy (all tokens): 1.5850
Overall mean entropy (last token): 1.9549
Overall mean entropy (last 16 tokens): 1.9698
Most focused layer (all tokens): L3 (entropy: 0.1611)

Results saved to: /home/ubuntu/CoTLab/outputs/2026-02-26/17-46-00_attention_analysis_medgemma_27b_text_it_radiology_json_radiology/results.json

Metrics:
  num_samples_analyzed: 100
  num_layers_analyzed: 62
  num_heads: 32
  overall_mean_entropy: 1.5849825503514912
  overall_mean_entropy_all_tokens: 1.5849825503514912
  overall_mean_entropy_last_token: 1.9548816133539575
  overall_mean_entropy_last_k_tokens: 1.9697768256407664
  last_k_tokens: 16
  most_focused_layer: 3
  most_focused_entropy: 0.1610571669714744
  most_focused_layer_all_tokens: 3
  most_focused_entropy_all_tokens: 0.1610571669714744
  most_focused_layer_last_token: 3
  most_focused_entropy_last_token: 0.19781819701705253
  analyze_generated_tokens: False

Experiment documentation updated: /home/ubuntu/CoTLab/outputs/2026-02-26/17-46-00_attention_analysis_medgemma_27b_text_it_radiology_json_radiology/EXPERIMENT.md

Experiment complete.
