============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.pubhealthbench.PubHealthBenchMCQPromptStrategy
  name: pubhealthbench
  output_format: json
dataset:
  _target_: cotlab.datasets.loaders.PubHealthBenchDataset
  name: pubhealthbench
  split: reviewed
experiment:
  _target_: cotlab.experiments.AttentionAnalysisExperiment
  name: attention_analysis
  description: Analyze attention patterns at critical layers
  all_layers: true
  target_layers: null
  layer_stride: 1
  force_eager_reload: false
  num_samples: null
  last_k_tokens: 16
  max_input_tokens: 1024
  analyze_generated_tokens: false
  generated_max_new_tokens: 16
  generated_do_sample: false
  generated_temperature: 0.7
  generated_top_p: 0.9
  question: Patient presents with chest pain, sweating, and shortness of breath. What
    is the diagnosis?
  batch_size: 16
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /home/ubuntu/CoTLab/outputs/2026-02-26/17-28-09_attention_analysis_medgemma_27b_text_it_pubhealthbench_json_pubhealthbench/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:07,  1.42it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.41it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.41it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:05,  1.40it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.42it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.43it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:04<00:02,  1.41it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.41it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.42it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.47it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.43it/s]
  Resolved device: cuda:0
Creating prompt strategy: pubhealthbench
Loading dataset: pubhealthbench
pubhealthbench/reviewed-00000-of-00001.p(…):   0%|                    | 0.00/3.88M [00:00<?, ?B/s]pubhealthbench/reviewed-00000-of-00001.p(…):   0%|                    | 0.00/3.88M [00:00<?, ?B/s]pubhealthbench/reviewed-00000-of-00001.p(…):   0%|                    | 0.00/3.88M [00:00<?, ?B/s]pubhealthbench/reviewed-00000-of-00001.p(…):   0%|                    | 0.00/3.88M [00:00<?, ?B/s]pubhealthbench/reviewed-00000-of-00001.p(…):   0%|                    | 0.00/3.88M [00:00<?, ?B/s]pubhealthbench/reviewed-00000-of-00001.p(…): 100%|███████████| 3.88M/3.88M [00:00<00:00, 4.76MB/s]
Creating experiment: attention_analysis
============================================================
Running experiment: attention_analysis
============================================================
Model: google/medgemma-27b-text-it
Attention heads: 32
All layers enabled: True
Layer stride: 1
Resolved layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
Max input tokens: 1024
Batch size: 16
Analyze generated tokens: False
Current attention implementation: sdpa
Switching attention implementation to 'eager' in-place...

Analyzing attention on 760 samples (batch_size=16)...
Processing batches:   0%|                                                  | 0/48 [00:00<?, ?it/s]Processing batches:   2%|▉                                         | 1/48 [00:06<04:59,  6.38s/it]Processing batches:   4%|█▊                                        | 2/48 [00:12<04:44,  6.18s/it]Processing batches:   6%|██▋                                       | 3/48 [00:18<04:34,  6.10s/it]Processing batches:   8%|███▌                                      | 4/48 [00:24<04:25,  6.05s/it]Processing batches:  10%|████▍                                     | 5/48 [00:30<04:21,  6.08s/it]Processing batches:  12%|█████▎                                    | 6/48 [00:36<04:13,  6.03s/it]Processing batches:  15%|██████▏                                   | 7/48 [00:42<04:07,  6.03s/it]Processing batches:  17%|███████                                   | 8/48 [00:48<04:01,  6.04s/it]Processing batches:  19%|███████▉                                  | 9/48 [00:54<03:57,  6.08s/it]Processing batches:  21%|████████▌                                | 10/48 [01:00<03:50,  6.08s/it]Processing batches:  23%|█████████▍                               | 11/48 [01:06<03:44,  6.07s/it]Processing batches:  25%|██████████▎                              | 12/48 [01:12<03:37,  6.05s/it]Processing batches:  27%|███████████                              | 13/48 [01:18<03:32,  6.06s/it]Processing batches:  29%|███████████▉                             | 14/48 [01:24<03:25,  6.05s/it]Processing batches:  31%|████████████▊                            | 15/48 [01:30<03:18,  6.03s/it]Processing batches:  33%|█████████████▋                           | 16/48 [01:37<03:14,  6.08s/it]Processing batches:  35%|██████████████▌                          | 17/48 [01:43<03:07,  6.05s/it]Processing batches:  38%|███████████████▍                         | 18/48 [01:49<03:02,  6.10s/it]Processing batches:  40%|████████████████▏                        | 19/48 [01:55<02:56,  6.09s/it]Processing batches:  42%|█████████████████                        | 20/48 [02:01<02:49,  6.05s/it]Processing batches:  44%|█████████████████▉                       | 21/48 [02:07<02:42,  6.03s/it]Processing batches:  46%|██████████████████▊                      | 22/48 [02:13<02:37,  6.04s/it]Processing batches:  48%|███████████████████▋                     | 23/48 [02:19<02:33,  6.15s/it]Processing batches:  50%|████████████████████▌                    | 24/48 [02:25<02:26,  6.10s/it]Processing batches:  52%|█████████████████████▎                   | 25/48 [02:32<02:21,  6.16s/it]Processing batches:  54%|██████████████████████▏                  | 26/48 [02:38<02:14,  6.10s/it]Processing batches:  56%|███████████████████████                  | 27/48 [02:44<02:08,  6.11s/it]Processing batches:  58%|███████████████████████▉                 | 28/48 [02:50<02:03,  6.18s/it]Processing batches:  60%|████████████████████████▊                | 29/48 [02:56<01:56,  6.11s/it]Processing batches:  62%|█████████████████████████▋               | 30/48 [03:02<01:50,  6.14s/it]Processing batches:  65%|██████████████████████████▍              | 31/48 [03:08<01:43,  6.11s/it]Processing batches:  67%|███████████████████████████▎             | 32/48 [03:14<01:37,  6.09s/it]Processing batches:  69%|████████████████████████████▏            | 33/48 [03:20<01:31,  6.08s/it]Processing batches:  71%|█████████████████████████████            | 34/48 [03:27<01:25,  6.11s/it]Processing batches:  73%|█████████████████████████████▉           | 35/48 [03:33<01:19,  6.10s/it]Processing batches:  75%|██████████████████████████████▊          | 36/48 [03:39<01:12,  6.06s/it]Processing batches:  77%|███████████████████████████████▌         | 37/48 [03:45<01:06,  6.04s/it]Processing batches:  79%|████████████████████████████████▍        | 38/48 [03:51<01:00,  6.04s/it]Processing batches:  81%|█████████████████████████████████▎       | 39/48 [03:57<00:54,  6.05s/it]Processing batches:  83%|██████████████████████████████████▏      | 40/48 [04:03<00:48,  6.06s/it]Processing batches:  85%|███████████████████████████████████      | 41/48 [04:09<00:42,  6.05s/it]Processing batches:  88%|███████████████████████████████████▉     | 42/48 [04:15<00:36,  6.05s/it]Processing batches:  90%|████████████████████████████████████▋    | 43/48 [04:21<00:30,  6.08s/it]Processing batches:  92%|█████████████████████████████████████▌   | 44/48 [04:27<00:24,  6.10s/it]Processing batches:  94%|██████████████████████████████████████▍  | 45/48 [04:34<00:18,  6.28s/it]Processing batches:  96%|███████████████████████████████████████▎ | 46/48 [04:40<00:12,  6.30s/it]Processing batches:  98%|████████████████████████████████████████▏| 47/48 [04:46<00:06,  6.28s/it]Processing batches: 100%|█████████████████████████████████████████| 48/48 [04:49<00:00,  5.29s/it]Processing batches: 100%|█████████████████████████████████████████| 48/48 [04:49<00:00,  6.04s/it]

======================================================================
ATTENTION ANALYSIS: Aggregated Statistics Across Samples
======================================================================
Layer    | LastTok μ  | AllTok μ   | Last16 μ   | AllTok σ   | Top Tokens
----------------------------------------------------------------------------------------------------------
L0       | 2.2021     | 2.7775     | 3.4168     | 0.0939     | '<bos>', ')",', 'According'
L1       | 0.7343     | 1.0701     | 1.3441     | 0.0272     | ' ', '<bos>', '.'
L2       | 1.4277     | 0.9824     | 1.2148     | 0.0234     | '<bos>', '.', ' '
L3       | 0.2524     | 0.1032     | 0.1089     | 0.0041     | '<bos>', ' are', 'The'
L4       | 0.6380     | 0.7994     | 0.8614     | 0.0142     | '<bos>', ' ', ' are'
L5       | 1.1361     | 0.8561     | 1.0874     | 0.0240     | '<bos>', ' in', ' are'
L6       | 1.5525     | 1.0543     | 1.3212     | 0.0170     | '<bos>', '.', ' '
L7       | 0.8417     | 1.1545     | 1.4338     | 0.0323     | '<bos>', '.', ' '
L8       | 1.4569     | 1.3568     | 1.6593     | 0.0437     | '<bos>', ' ', 'The'
L9       | 1.5901     | 1.2386     | 1.3377     | 0.0244     | '<bos>', ' ', '.'
L10      | 1.9677     | 1.3218     | 1.5942     | 0.0308     | '<bos>', ' ', ' Agency'
L11      | 1.3408     | 1.1197     | 1.4840     | 0.0289     | '<bos>', 'The', ' '
L12      | 1.1856     | 0.7529     | 0.9128     | 0.0213     | '<bos>', 'The', '

'
L13      | 1.4313     | 1.0538     | 1.3879     | 0.0295     | '<bos>', ' Government', ' '
L14      | 1.4783     | 1.0834     | 1.2015     | 0.0361     | '<bos>', 'The', ' '
L15      | 1.5402     | 1.1175     | 1.3283     | 0.0311     | '<bos>', ' include', ' should'
L16      | 1.6089     | 1.2662     | 1.5779     | 0.0341     | '<bos>', ' your', ' '
L17      | 2.1721     | 1.6434     | 2.1060     | 0.0515     | ' ', ' the', '<bos>'
L18      | 2.4845     | 2.1087     | 2.6319     | 0.0495     | '<bos>', '.', '

'
L19      | 1.7297     | 1.5952     | 2.0069     | 0.0390     | '<bos>', ' ', ' information'
L20      | 2.2153     | 1.7874     | 2.2022     | 0.0479     | '<bos>', ' ', ':'
L21      | 1.9657     | 1.8468     | 2.2318     | 0.0584     | ' ', '<bos>', ':'
L22      | 2.2892     | 1.8971     | 2.3421     | 0.0460     | '<bos>', ' ', 'The'
L23      | 2.1177     | 2.1116     | 2.5366     | 0.0628     | '<bos>', ' ', ' guidance'
L24      | 2.1850     | 2.2512     | 2.6182     | 0.0513     | '<bos>', ' ', ':'
L25      | 2.3278     | 2.3996     | 2.7947     | 0.0565     | ' ', ' the', '<bos>'
L26      | 2.3755     | 2.3604     | 2.7730     | 0.0593     | '<bos>', ' ', ':'
L27      | 1.9710     | 2.2660     | 2.8009     | 0.0596     | '<bos>', ' ', 'The'
L28      | 2.2244     | 2.2763     | 2.7131     | 0.0517     | '<bos>', ' ', 'The'
L29      | 1.5341     | 1.8425     | 2.3441     | 0.0503     | '<bos>', ' ', 'The'
L30      | 1.7429     | 1.9371     | 2.4103     | 0.0494     | '<bos>', ' ', ' the'
L31      | 1.2599     | 1.7161     | 2.0534     | 0.0472     | '<bos>', ' ', ':'
L32      | 1.2909     | 1.5976     | 1.8175     | 0.0327     | '<bos>', ' ', ':'
L33      | 1.2253     | 1.5676     | 1.9010     | 0.0416     | '<bos>', ' ', 'The'
L34      | 1.1771     | 1.4096     | 1.5762     | 0.0301     | '<bos>', ' ', '

'
L35      | 1.2975     | 1.6783     | 2.2674     | 0.0625     | '<bos>', ' ', 'The'
L36      | 1.3433     | 1.7116     | 1.8478     | 0.0477     | '<bos>', ' ', ':'
L37      | 1.1752     | 1.5675     | 2.0084     | 0.0457     | '<bos>', ' ', ':'
L38      | 0.8783     | 1.1846     | 1.2561     | 0.0275     | '<bos>', ' ', 'The'
L39      | 0.7244     | 1.0843     | 1.1875     | 0.0334     | '<bos>', ' ', ':'
L40      | 0.3619     | 0.4317     | 0.5938     | 0.0195     | '<bos>', 'Answer', ' '
L41      | 0.5608     | 0.7305     | 1.1627     | 0.0298     | '<bos>', 'The', ' '
L42      | 0.7204     | 1.0053     | 1.2305     | 0.0445     | '<bos>', ' ', '

'
L43      | 0.5521     | 0.6617     | 0.8211     | 0.0201     | '<bos>', ' ', ' NOT'
L44      | 0.4647     | 0.6651     | 0.7236     | 0.0193     | '<bos>', ' ', 'Answer'
L45      | 0.5033     | 0.5825     | 0.6499     | 0.0240     | '<bos>', ' ', '

'
L46      | 0.5795     | 0.7057     | 0.8047     | 0.0225     | '<bos>', ' ', 'Answer'
L47      | 0.8269     | 1.1468     | 1.7136     | 0.0599     | '<bos>', ' ', 'H'
L48      | 0.3699     | 0.3062     | 0.3856     | 0.0114     | '<bos>', ' ', 'Answer'
L49      | 0.3326     | 0.3032     | 0.3658     | 0.0119     | '<bos>', ' ', 'Answer'
L50      | 0.3983     | 0.3256     | 0.3584     | 0.0121     | '<bos>', ' ', 'The'
L51      | 0.4474     | 0.5097     | 0.6005     | 0.0193     | '<bos>', ' ', ' NOT'
L52      | 0.8433     | 0.8191     | 0.8906     | 0.0219     | '<bos>', ' or', ' '
L53      | 0.7635     | 1.0899     | 1.4415     | 0.0474     | '<bos>', ' ', '
'
L54      | 1.2235     | 1.2964     | 1.4378     | 0.0322     | '<bos>', ':', ' '
L55      | 1.2770     | 1.2138     | 1.4387     | 0.0413     | '<bos>', 'Answer', 'The'
L56      | 1.0004     | 1.0662     | 1.1209     | 0.0261     | '<bos>', ':', ' '
L57      | 0.5246     | 0.4541     | 0.5277     | 0.0151     | '<bos>', 'The', ' '
L58      | 1.7176     | 1.5924     | 1.7432     | 0.0402     | '<bos>', ' ', ':'
L59      | 1.7222     | 1.1494     | 1.5541     | 0.0502     | '<bos>', ' ', '

'
L60      | 0.9954     | 1.0117     | 1.0081     | 0.0249     | '<bos>', ' ', ':'
L61      | 0.9417     | 1.0651     | 1.1236     | 0.0261     | '<bos>', ' ', ':'
----------------------------------------------------------------------------------------------------------

Overall mean entropy (all tokens): 1.2755
Overall mean entropy (last token): 1.2777
Overall mean entropy (last 16 tokens): 1.5386
Most focused layer (all tokens): L3 (entropy: 0.1032)

Results saved to: /home/ubuntu/CoTLab/outputs/2026-02-26/17-28-09_attention_analysis_medgemma_27b_text_it_pubhealthbench_json_pubhealthbench/results.json

Metrics:
  num_samples_analyzed: 760
  num_layers_analyzed: 62
  num_heads: 32
  overall_mean_entropy: 1.2755012371047267
  overall_mean_entropy_all_tokens: 1.2755012371047267
  overall_mean_entropy_last_token: 1.277712745345147
  overall_mean_entropy_last_k_tokens: 1.5386412371347047
  last_k_tokens: 16
  most_focused_layer: 3
  most_focused_entropy: 0.1032212932338464
  most_focused_layer_all_tokens: 3
  most_focused_entropy_all_tokens: 0.1032212932338464
  most_focused_layer_last_token: 3
  most_focused_entropy_last_token: 0.25241097626511044
  analyze_generated_tokens: False

Experiment documentation updated: /home/ubuntu/CoTLab/outputs/2026-02-26/17-28-09_attention_analysis_medgemma_27b_text_it_pubhealthbench_json_pubhealthbench/EXPERIMENT.md

Experiment complete.
