============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.mcq.MCQPromptStrategy
  name: mcq
  few_shot: true
  output_format: json
  answer_first: false
  contrarian: false
dataset:
  _target_: cotlab.datasets.loaders.MedQADataset
  name: afrimedqa
  filename: afrimedqa/mcq.jsonl
  split: mcq
experiment:
  _target_: cotlab.experiments.ActivationPatchingExperiment
  name: activation_patching
  description: Layer-wise causal activation patching (logit recovery)
  patching_mode: introspect_contrast
  layer_stride: 2
  num_samples: 50
  max_input_tokens: 1024
  seed: 42
  answer_cue: '


    Answer:'
  introspect_instruction: Think deeply about this problem. Carefully reason through
    the underlying mechanisms and consider all relevant factors before committing
    to your answer.
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /home/ubuntu/CoTLab/outputs/2026-02-26/19-02-03_activation_patching_medgemma_27b_text_it_mcq_json_afrimedqa/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:07,  1.40it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.39it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.39it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:05,  1.38it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.40it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.41it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:05<00:02,  1.39it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.39it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.41it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.46it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.42it/s]
  Resolved device: cuda:0
Creating prompt strategy: mcq
Loading dataset: afrimedqa
Creating experiment: activation_patching
============================================================
Running experiment: activation_patching
============================================================
Model        : google/medgemma-27b-text-it
Patching mode: introspect_contrast
Layers (31): [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60]
Stride : 2  |  max_input_tokens: 1024
Samples: 50  (each requires 33 forward passes)

Activation patching:   0%|                                                 | 0/50 [00:00<?, ?it/s]Activation patching:   2%|▊                                        | 1/50 [00:04<03:31,  4.31s/it]Activation patching:   4%|█▋                                       | 2/50 [00:08<03:20,  4.17s/it]Activation patching:   6%|██▍                                      | 3/50 [00:12<03:13,  4.12s/it]Activation patching:   8%|███▎                                     | 4/50 [00:16<03:09,  4.12s/it]Activation patching:  10%|████                                     | 5/50 [00:20<03:04,  4.10s/it]Activation patching:  12%|████▉                                    | 6/50 [00:24<02:59,  4.09s/it]Activation patching:  14%|█████▋                                   | 7/50 [00:28<02:57,  4.14s/it]Activation patching:  16%|██████▌                                  | 8/50 [00:33<02:53,  4.13s/it]Activation patching:  18%|███████▍                                 | 9/50 [00:37<02:50,  4.15s/it]Activation patching:  20%|████████                                | 10/50 [00:41<02:44,  4.12s/it]Activation patching:  22%|████████▊                               | 11/50 [00:45<02:39,  4.10s/it]Activation patching:  24%|█████████▌                              | 12/50 [00:49<02:36,  4.11s/it]Activation patching:  26%|██████████▍                             | 13/50 [00:53<02:32,  4.12s/it]Activation patching:  28%|███████████▏                            | 14/50 [00:57<02:27,  4.11s/it]Activation patching:  30%|████████████                            | 15/50 [01:02<02:28,  4.24s/it]Activation patching:  32%|████████████▊                           | 16/50 [01:06<02:22,  4.19s/it]Activation patching:  34%|█████████████▌                          | 17/50 [01:10<02:17,  4.18s/it]Activation patching:  36%|██████████████▍                         | 18/50 [01:15<02:19,  4.36s/it]Activation patching:  38%|███████████████▏                        | 19/50 [01:19<02:16,  4.42s/it]Activation patching:  40%|████████████████                        | 20/50 [01:23<02:09,  4.33s/it]Activation patching:  42%|████████████████▊                       | 21/50 [01:28<02:03,  4.27s/it]Activation patching:  44%|█████████████████▌                      | 22/50 [01:32<01:57,  4.21s/it]Activation patching:  46%|██████████████████▍                     | 23/50 [01:36<01:53,  4.22s/it]Activation patching:  48%|███████████████████▏                    | 24/50 [01:41<01:52,  4.34s/it]Activation patching:  50%|████████████████████                    | 25/50 [01:45<01:46,  4.27s/it]Activation patching:  52%|████████████████████▊                   | 26/50 [01:49<01:41,  4.24s/it]Activation patching:  54%|█████████████████████▌                  | 27/50 [01:53<01:39,  4.32s/it]Activation patching:  56%|██████████████████████▍                 | 28/50 [01:57<01:33,  4.25s/it]Activation patching:  58%|███████████████████████▏                | 29/50 [02:01<01:28,  4.21s/it]Activation patching:  60%|████████████████████████                | 30/50 [02:06<01:23,  4.20s/it]Activation patching:  62%|████████████████████████▊               | 31/50 [02:10<01:19,  4.17s/it]Activation patching:  64%|█████████████████████████▌              | 32/50 [02:14<01:14,  4.15s/it]Activation patching:  66%|██████████████████████████▍             | 33/50 [02:18<01:12,  4.26s/it]Activation patching:  68%|███████████████████████████▏            | 34/50 [02:23<01:07,  4.23s/it]Activation patching:  70%|████████████████████████████            | 35/50 [02:27<01:02,  4.19s/it]Activation patching:  72%|████████████████████████████▊           | 36/50 [02:31<00:58,  4.15s/it]Activation patching:  74%|█████████████████████████████▌          | 37/50 [02:35<00:54,  4.19s/it]Activation patching:  76%|██████████████████████████████▍         | 38/50 [02:39<00:50,  4.19s/it]Activation patching:  78%|███████████████████████████████▏        | 39/50 [02:43<00:45,  4.16s/it]Activation patching:  80%|████████████████████████████████        | 40/50 [02:47<00:41,  4.14s/it]Activation patching:  82%|████████████████████████████████▊       | 41/50 [02:51<00:37,  4.11s/it]Activation patching:  84%|█████████████████████████████████▌      | 42/50 [02:56<00:33,  4.17s/it]Activation patching:  86%|██████████████████████████████████▍     | 43/50 [03:00<00:30,  4.29s/it]Activation patching:  88%|███████████████████████████████████▏    | 44/50 [03:04<00:25,  4.24s/it]Activation patching:  90%|████████████████████████████████████    | 45/50 [03:08<00:20,  4.18s/it]Activation patching:  92%|████████████████████████████████████▊   | 46/50 [03:12<00:16,  4.14s/it]Activation patching:  94%|█████████████████████████████████████▌  | 47/50 [03:17<00:12,  4.13s/it]Activation patching:  96%|██████████████████████████████████████▍ | 48/50 [03:21<00:08,  4.18s/it]Activation patching:  98%|███████████████████████████████████████▏| 49/50 [03:26<00:04,  4.32s/it]Activation patching: 100%|████████████████████████████████████████| 50/50 [03:30<00:00,  4.29s/it]Activation patching: 100%|████████████████████████████████████████| 50/50 [03:30<00:00,  4.21s/it]

==============================================================
ACTIVATION PATCHING SUMMARY  (logit-recovery effect)
==============================================================
Processed samples : 50 / 50
Top-5 causal layers: [0, 20, 32, 12, 56]

 Layer   Mean Effect   N samples
----------------------------------
     0        0.8000          50
     2        0.2581          50
     4        0.1087          50
     6        0.0921          50
     8        0.1404          50
    10        0.2528          50
    12        0.5622          50
    14        0.1099          50
    16        0.1327          50
    18        0.1520          50
    20        0.6686          50
    22        0.1792          50
    24        0.1552          50
    26        0.1400          50
    28        0.1480          50
    30        0.4225          50
    32        0.5627          50
    34        0.0963          50
    36        0.1201          50
    38        0.4276          50
    40        0.0931          50
    42        0.1043          50
    44        0.1668          50
    46        0.0831          50
    48        0.0471          50
    50        0.1177          50
    52        0.1633          50
    54        0.4660          50
    56        0.4848          50
    58        0.2415          50
    60       -0.0276          50
==============================================================

Results saved to: /home/ubuntu/CoTLab/outputs/2026-02-26/19-02-03_activation_patching_medgemma_27b_text_it_mcq_json_afrimedqa/results.json

Metrics:
  num_samples: 50
  layer_stride: 2
  mean_effect_per_layer: {0: 0.8, 2: 0.2581, 4: 0.1087, 6: 0.0921, 8: 0.1404, 10: 0.2528, 12: 0.5622, 14: 0.1099, 16: 0.1327, 18: 0.152, 20: 0.6686, 22: 0.1792, 24: 0.1552, 26: 0.14, 28: 0.148, 30: 0.4225, 32: 0.5627, 34: 0.0963, 36: 0.1201, 38: 0.4276, 40: 0.0931, 42: 0.1043, 44: 0.1668, 46: 0.0831, 48: 0.0471, 50: 0.1177, 52: 0.1633, 54: 0.466, 56: 0.4848, 58: 0.2415, 60: -0.0276}
  top_5_causal_layers: [0, 20, 32, 12, 56]

Experiment documentation updated: /home/ubuntu/CoTLab/outputs/2026-02-26/19-02-03_activation_patching_medgemma_27b_text_it_mcq_json_afrimedqa/EXPERIMENT.md

Experiment complete.
