============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.mcq.MCQPromptStrategy
  name: mcq
  few_shot: true
  output_format: json
  answer_first: false
  contrarian: false
dataset:
  _target_: cotlab.datasets.loaders.MARCDataset
  name: m_arc
  filename: m_arc/test-00000-of-00001.parquet
  split: test
experiment:
  _target_: cotlab.experiments.AttentionAnalysisExperiment
  name: attention_analysis
  description: Analyze attention patterns at critical layers
  all_layers: true
  target_layers: null
  layer_stride: 1
  force_eager_reload: false
  num_samples: null
  last_k_tokens: 16
  max_input_tokens: 1024
  analyze_generated_tokens: false
  generated_max_new_tokens: 16
  generated_do_sample: false
  generated_temperature: 0.7
  generated_top_p: 0.9
  question: Patient presents with chest pain, sweating, and shortness of breath. What
    is the diagnosis?
  batch_size: 16
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /root/CoTLab/CoTLab/outputs/2026-02-26/16-09-59_attention_analysis_medgemma_27b_text_it_mcq_json_m_arc/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:06,  1.47it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.44it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.43it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:04,  1.43it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.43it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.42it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:04<00:02,  1.42it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.42it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.42it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.43it/s]
  Resolved device: cuda:0
Creating prompt strategy: mcq
Loading dataset: m_arc
m_arc/test-00000-of-00001.parquet:   0%|                              | 0.00/49.3k [00:00<?, ?B/s]m_arc/test-00000-of-00001.parquet:   0%|                              | 0.00/49.3k [00:00<?, ?B/s]m_arc/test-00000-of-00001.parquet:   0%|                              | 0.00/49.3k [00:00<?, ?B/s]m_arc/test-00000-of-00001.parquet:   0%|                              | 0.00/49.3k [00:00<?, ?B/s]m_arc/test-00000-of-00001.parquet: 100%|█████████████████████| 49.3k/49.3k [00:00<00:00, 67.8kB/s]
Creating experiment: attention_analysis
============================================================
Running experiment: attention_analysis
============================================================
Model: google/medgemma-27b-text-it
Attention heads: 32
All layers enabled: True
Layer stride: 1
Resolved layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
Max input tokens: 1024
Batch size: 16
Analyze generated tokens: False
Current attention implementation: sdpa
Switching attention implementation to 'eager' in-place...

Analyzing attention on 100 samples (batch_size=16)...
Processing batches:   0%|                                                   | 0/7 [00:00<?, ?it/s]Processing batches:  14%|██████▏                                    | 1/7 [00:06<00:41,  6.87s/it]Processing batches:  29%|████████████▎                              | 2/7 [00:13<00:33,  6.70s/it]Processing batches:  43%|██████████████████▍                        | 3/7 [00:20<00:27,  6.88s/it]Processing batches:  57%|████████████████████████▌                  | 4/7 [00:27<00:20,  6.96s/it]Processing batches:  71%|██████████████████████████████▋            | 5/7 [00:34<00:14,  7.00s/it]Processing batches:  86%|████████████████████████████████████▊      | 6/7 [00:41<00:06,  6.90s/it]Processing batches: 100%|███████████████████████████████████████████| 7/7 [00:43<00:00,  5.23s/it]Processing batches: 100%|███████████████████████████████████████████| 7/7 [00:43<00:00,  6.17s/it]

======================================================================
ATTENTION ANALYSIS: Aggregated Statistics Across Samples
======================================================================
Layer    | LastTok μ  | AllTok μ   | Last16 μ   | AllTok σ   | Top Tokens
----------------------------------------------------------------------------------------------------------
L0       | 3.7683     | 3.4080     | 4.1654     | 0.0935     | '.', ',', ' of'
L1       | 1.7157     | 1.4455     | 1.8317     | 0.0958     | ' ', '<bos>', '.'
L2       | 1.7292     | 1.4154     | 2.0108     | 0.1332     | ' ', '.', '<bos>'
L3       | 0.5397     | 0.2830     | 0.5347     | 0.0743     | ' ', '<bos>', '.'
L4       | 1.1299     | 0.9563     | 1.2016     | 0.0621     | ' ', '<bos>', '.'
L5       | 1.4624     | 1.0929     | 1.3646     | 0.0481     | ' ', '<bos>', '''
L6       | 2.0177     | 1.5153     | 2.0732     | 0.1407     | ' ', '<bos>', ' ...)'
L7       | 2.0687     | 1.5420     | 1.9883     | 0.0882     | ' ', '<bos>', '.'
L8       | 2.2755     | 1.7311     | 2.1540     | 0.1070     | '.', ' answer', ' '
L9       | 1.9843     | 1.5774     | 2.0698     | 0.1016     | '.', ' answer', '
'
L10      | 2.3261     | 1.6560     | 2.3661     | 0.1111     | ''', '<bos>', ' '
L11      | 1.6861     | 1.4808     | 1.9327     | 0.0814     | ''', '<bos>', ' '
L12      | 1.8453     | 1.2454     | 1.8601     | 0.1181     | ' ', '<bos>', '''
L13      | 2.3031     | 1.5167     | 2.1297     | 0.1111     | ' ', '<bos>', '''
L14      | 2.2486     | 1.5872     | 2.0642     | 0.1165     | '.', '
', ':'
L15      | 2.2795     | 1.4958     | 2.0291     | 0.0881     | ' ', ''', '<bos>'
L16      | 2.0070     | 1.6085     | 1.9753     | 0.0674     | ' ', '<bos>', '''
L17      | 2.6504     | 2.0518     | 2.5796     | 0.0840     | '<bos>', ' ', '''
L18      | 3.2033     | 2.5504     | 3.1809     | 0.0864     | '.', ' answer', ','
L19      | 2.6348     | 1.9764     | 2.5159     | 0.0817     | ' ', '<bos>', '''
L20      | 2.8847     | 2.1313     | 2.7317     | 0.0818     | '.', ' answer', ' D'
L21      | 2.9209     | 2.3252     | 2.7693     | 0.0883     | '.', ' answer', '```'
L22      | 3.0818     | 2.3120     | 2.8035     | 0.0781     | '.', ' answer', ' of'
L23      | 3.1320     | 2.5740     | 2.9859     | 0.0975     | '<bos>', '```', ' '
L24      | 3.4147     | 2.6353     | 3.2390     | 0.0824     | '.', ' answer', '"}'
L25      | 3.4782     | 2.7519     | 3.1792     | 0.0684     | '
', '.', ' answer'
L26      | 3.5136     | 2.7498     | 3.2981     | 0.0815     | '.', '```', '<bos>'
L27      | 3.3351     | 2.7182     | 3.1467     | 0.0883     | ':', '.', ' ...)'
L28      | 3.3752     | 2.6723     | 3.2425     | 0.0807     | ' ', '<bos>', '''
L29      | 2.7447     | 2.2299     | 2.7261     | 0.0832     | '<bos>', ' ', '```'
L30      | 2.8260     | 2.2989     | 2.8077     | 0.0836     | ' ', '<bos>', '",'
L31      | 2.8228     | 2.0601     | 2.5645     | 0.0901     | ' ', '<bos>', '```'
L32      | 2.5361     | 1.8538     | 2.3790     | 0.0915     | ' ', '<bos>', ' format'
L33      | 2.7276     | 1.8774     | 2.4494     | 0.1011     | ' ', '<bos>', ' letter'
L34      | 2.2881     | 1.6292     | 2.0922     | 0.0948     | ' ', '<bos>', ' answer'
L35      | 2.9200     | 2.0772     | 2.6826     | 0.1050     | '<bos>', ' ', '```'
L36      | 2.6349     | 1.9926     | 2.4154     | 0.0885     | ' ', '<bos>', '.'
L37      | 2.7197     | 1.9173     | 2.6165     | 0.0784     | ' ', '<bos>', 'g'
L38      | 2.3034     | 1.4349     | 1.9362     | 0.0938     | ' ', '<bos>', '.'
L39      | 2.1151     | 1.3264     | 1.8062     | 0.0920     | ' ', '<bos>', ' "'
L40      | 1.3714     | 0.6708     | 1.2217     | 0.1168     | ' ', '<bos>', ' answer'
L41      | 1.8343     | 0.9658     | 1.5731     | 0.0899     | ' ', '<bos>', '##'
L42      | 2.2791     | 1.3454     | 1.9473     | 0.1014     | ' ', '<bos>', 'json'
L43      | 1.4560     | 0.9387     | 1.4835     | 0.1103     | ' ', '<bos>', ' correct'
L44      | 1.5010     | 0.9705     | 1.4196     | 0.1065     | ' ', '<bos>', '.'
L45      | 1.6035     | 0.8916     | 1.4223     | 0.1109     | ' ', '<bos>', ' "'
L46      | 1.5025     | 0.9727     | 1.5287     | 0.1066     | ' ', '<bos>', '.'
L47      | 2.3235     | 1.4167     | 2.3568     | 0.1087     | '<bos>', ' ', '.'
L48      | 1.2025     | 0.5839     | 1.2430     | 0.1197     | ' ', '<bos>', ' correct'
L49      | 1.1994     | 0.5807     | 1.1463     | 0.1179     | ' ', '<bos>', ' answer'
L50      | 1.0908     | 0.6004     | 1.1310     | 0.1162     | ' ', '<bos>', '.'
L51      | 1.4053     | 0.8075     | 1.3826     | 0.1133     | ' ', '<bos>', ' answer'
L52      | 1.6838     | 1.0737     | 1.5557     | 0.1053     | ' ', '<bos>', '.'
L53      | 2.3043     | 1.3641     | 2.0760     | 0.1071     | '<bos>', ' ', '
'
L54      | 2.1657     | 1.6144     | 2.0604     | 0.0991     | ' ', '<bos>', ' answer'
L55      | 2.3294     | 1.5001     | 2.1116     | 0.1146     | ' ', '<bos>', '''
L56      | 1.8480     | 1.3244     | 1.7370     | 0.1010     | ' ', '<bos>', ' answer'
L57      | 1.2545     | 0.7092     | 1.1779     | 0.1152     | ' ', '<bos>', '.'
L58      | 2.6517     | 1.7941     | 2.2772     | 0.1076     | ' ', '<bos>', '.'
L59      | 2.4075     | 1.3714     | 2.0863     | 0.1310     | '<bos>', ' ', '

'
L60      | 1.7756     | 1.2436     | 1.6722     | 0.0994     | ' ', '<bos>', '.'
L61      | 1.8235     | 1.3099     | 1.6607     | 0.0818     | '<bos>', '.', ' '
----------------------------------------------------------------------------------------------------------

Overall mean entropy (all tokens): 1.6089
Overall mean entropy (last token): 2.2365
Overall mean entropy (last 16 tokens): 2.1318
Most focused layer (all tokens): L3 (entropy: 0.2830)

Results saved to: /root/CoTLab/CoTLab/outputs/2026-02-26/16-09-59_attention_analysis_medgemma_27b_text_it_mcq_json_m_arc/results.json

Metrics:
  num_samples_analyzed: 100
  num_layers_analyzed: 62
  num_heads: 32
  overall_mean_entropy: 1.6089222817731417
  overall_mean_entropy_all_tokens: 1.6089222817731417
  overall_mean_entropy_last_token: 2.2365003354805943
  overall_mean_entropy_last_k_tokens: 2.131777534496532
  last_k_tokens: 16
  most_focused_layer: 3
  most_focused_entropy: 0.28298126966758114
  most_focused_layer_all_tokens: 3
  most_focused_entropy_all_tokens: 0.28298126966758114
  most_focused_layer_last_token: 3
  most_focused_entropy_last_token: 0.5396548049939901
  analyze_generated_tokens: False

Experiment documentation updated: /root/CoTLab/CoTLab/outputs/2026-02-26/16-09-59_attention_analysis_medgemma_27b_text_it_mcq_json_m_arc/EXPERIMENT.md

Experiment complete.
