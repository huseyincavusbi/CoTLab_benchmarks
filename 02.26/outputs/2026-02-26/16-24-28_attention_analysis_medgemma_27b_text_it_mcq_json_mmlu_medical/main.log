============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.mcq.MCQPromptStrategy
  name: mcq
  few_shot: true
  output_format: json
  answer_first: false
  contrarian: false
dataset:
  _target_: cotlab.datasets.loaders.MMLUMedicalDataset
  name: mmlu_medical
  filename: mmlu/medical_test.jsonl
experiment:
  _target_: cotlab.experiments.AttentionAnalysisExperiment
  name: attention_analysis
  description: Analyze attention patterns at critical layers
  all_layers: true
  target_layers: null
  layer_stride: 1
  force_eager_reload: false
  num_samples: null
  last_k_tokens: 16
  max_input_tokens: 1024
  analyze_generated_tokens: false
  generated_max_new_tokens: 16
  generated_do_sample: false
  generated_temperature: 0.7
  generated_top_p: 0.9
  question: Patient presents with chest pain, sweating, and shortness of breath. What
    is the diagnosis?
  batch_size: 16
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /root/CoTLab/CoTLab/outputs/2026-02-26/16-24-28_attention_analysis_medgemma_27b_text_it_mcq_json_mmlu_medical/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:06,  1.47it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.44it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.43it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:04,  1.43it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.43it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.42it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:04<00:02,  1.42it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.41it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.42it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.43it/s]
  Resolved device: cuda:0
Creating prompt strategy: mcq
Loading dataset: mmlu_medical
medical_test.jsonl: 0.00B [00:00, ?B/s]medical_test.jsonl: 196kB [00:00, 5.84MB/s]
Creating experiment: attention_analysis
============================================================
Running experiment: attention_analysis
============================================================
Model: google/medgemma-27b-text-it
Attention heads: 32
All layers enabled: True
Layer stride: 1
Resolved layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
Max input tokens: 1024
Batch size: 16
Analyze generated tokens: False
Current attention implementation: sdpa
Switching attention implementation to 'eager' in-place...

Analyzing attention on 644 samples (batch_size=16)...
Processing batches:   0%|                                                  | 0/41 [00:00<?, ?it/s]Processing batches:   2%|█                                         | 1/41 [00:06<04:24,  6.61s/it]Processing batches:   5%|██                                        | 2/41 [00:12<04:09,  6.39s/it]Processing batches:   7%|███                                       | 3/41 [00:19<03:59,  6.31s/it]Processing batches:  10%|████                                      | 4/41 [00:25<03:51,  6.27s/it]Processing batches:  12%|█████                                     | 5/41 [00:31<03:46,  6.29s/it]Processing batches:  15%|██████▏                                   | 6/41 [00:37<03:38,  6.24s/it]Processing batches:  17%|███████▏                                  | 7/41 [00:43<03:32,  6.24s/it]Processing batches:  20%|████████▏                                 | 8/41 [00:50<03:25,  6.22s/it]Processing batches:  22%|█████████▏                                | 9/41 [00:56<03:18,  6.20s/it]Processing batches:  24%|██████████                               | 10/41 [01:02<03:12,  6.22s/it]Processing batches:  27%|███████████                              | 11/41 [01:08<03:06,  6.21s/it]Processing batches:  29%|████████████                             | 12/41 [01:14<02:59,  6.20s/it]Processing batches:  32%|█████████████                            | 13/41 [01:21<02:53,  6.20s/it]Processing batches:  34%|██████████████                           | 14/41 [01:27<02:47,  6.19s/it]Processing batches:  37%|███████████████                          | 15/41 [01:33<02:40,  6.19s/it]Processing batches:  39%|████████████████                         | 16/41 [01:39<02:34,  6.18s/it]Processing batches:  41%|█████████████████                        | 17/41 [01:45<02:28,  6.18s/it]Processing batches:  44%|██████████████████                       | 18/41 [01:52<02:23,  6.23s/it]Processing batches:  46%|███████████████████                      | 19/41 [01:58<02:17,  6.24s/it]Processing batches:  49%|████████████████████                     | 20/41 [02:04<02:11,  6.25s/it]Processing batches:  51%|█████████████████████                    | 21/41 [02:10<02:04,  6.24s/it]Processing batches:  54%|██████████████████████                   | 22/41 [02:17<01:58,  6.24s/it]Processing batches:  56%|███████████████████████                  | 23/41 [02:23<01:52,  6.23s/it]Processing batches:  59%|████████████████████████                 | 24/41 [02:29<01:45,  6.22s/it]Processing batches:  61%|█████████████████████████                | 25/41 [02:35<01:39,  6.21s/it]Processing batches:  63%|██████████████████████████               | 26/41 [02:41<01:32,  6.19s/it]Processing batches:  66%|███████████████████████████              | 27/41 [02:48<01:26,  6.18s/it]Processing batches:  68%|████████████████████████████             | 28/41 [02:54<01:20,  6.18s/it]Processing batches:  71%|█████████████████████████████            | 29/41 [03:00<01:14,  6.18s/it]Processing batches:  73%|██████████████████████████████           | 30/41 [03:06<01:07,  6.18s/it]Processing batches:  76%|███████████████████████████████          | 31/41 [03:12<01:01,  6.17s/it]Processing batches:  78%|████████████████████████████████         | 32/41 [03:18<00:55,  6.18s/it]Processing batches:  80%|█████████████████████████████████        | 33/41 [03:25<00:49,  6.24s/it]Processing batches:  83%|██████████████████████████████████       | 34/41 [03:31<00:44,  6.29s/it]Processing batches:  85%|███████████████████████████████████      | 35/41 [03:38<00:37,  6.30s/it]Processing batches:  88%|████████████████████████████████████     | 36/41 [03:44<00:31,  6.29s/it]Processing batches:  90%|█████████████████████████████████████    | 37/41 [03:50<00:25,  6.30s/it]Processing batches:  93%|██████████████████████████████████████   | 38/41 [03:57<00:18,  6.31s/it]Processing batches:  95%|███████████████████████████████████████  | 39/41 [04:03<00:12,  6.31s/it]Processing batches:  98%|████████████████████████████████████████ | 40/41 [04:09<00:06,  6.30s/it]Processing batches: 100%|█████████████████████████████████████████| 41/41 [04:11<00:00,  4.87s/it]Processing batches: 100%|█████████████████████████████████████████| 41/41 [04:11<00:00,  6.13s/it]

======================================================================
ATTENTION ANALYSIS: Aggregated Statistics Across Samples
======================================================================
Layer    | LastTok μ  | AllTok μ   | Last16 μ   | AllTok σ   | Top Tokens
----------------------------------------------------------------------------------------------------------
L0       | 3.5347     | 3.2432     | 4.0092     | 0.0318     | '.', ',', ' of'
L1       | 1.3631     | 1.2991     | 1.4898     | 0.0183     | ' ', '<bos>', '''
L2       | 1.0078     | 1.2264     | 1.5181     | 0.0307     | ' ', '<bos>', '''
L3       | 0.2537     | 0.1855     | 0.2844     | 0.0158     | ' ', '<bos>', '''
L4       | 0.9983     | 0.8882     | 1.0771     | 0.0063     | ' ', '<bos>', '''
L5       | 1.4001     | 1.0226     | 1.2882     | 0.0085     | ' ', '<bos>', ' in'
L6       | 1.5769     | 1.3090     | 1.6785     | 0.0334     | ' ', '<bos>', '''
L7       | 1.7910     | 1.4123     | 1.7266     | 0.0132     | ' ', '<bos>', '''
L8       | 1.9105     | 1.5771     | 1.8418     | 0.0176     | ' ', '<bos>', '''
L9       | 1.6060     | 1.4048     | 1.6918     | 0.0221     | ''', '<bos>', '

'
L10      | 2.0460     | 1.4781     | 2.0258     | 0.0282     | ''', '<bos>', '

'
L11      | 1.4783     | 1.3511     | 1.7241     | 0.0175     | ''', '<bos>', '##'
L12      | 1.3775     | 1.0251     | 1.3990     | 0.0380     | ''', '<bos>', ' D'
L13      | 1.9763     | 1.2984     | 1.7020     | 0.0341     | ''', '<bos>', 'Your'
L14      | 2.0283     | 1.3868     | 1.7258     | 0.0269     | ':', '.', '
'
L15      | 2.0561     | 1.3318     | 1.7047     | 0.0246     | ''', '<bos>', ' select'
L16      | 1.8297     | 1.4821     | 1.7665     | 0.0157     | ''', '<bos>', '

'
L17      | 2.5496     | 1.8974     | 2.4297     | 0.0147     | '<bos>', ' Instructions', '''
L18      | 3.0144     | 2.3710     | 2.9475     | 0.0178     | '.', ' answer', ','
L19      | 2.4693     | 1.8213     | 2.2810     | 0.0201     | ''', '<bos>', ' JSON'
L20      | 2.7556     | 1.9846     | 2.5631     | 0.0205     | '.', ' answer', ' D'
L21      | 2.7813     | 2.1547     | 2.5557     | 0.0191     | '.', ' answer', '```'
L22      | 2.9973     | 2.1629     | 2.5914     | 0.0180     | '.', ' answer', ' of'
L23      | 2.9890     | 2.4079     | 2.8386     | 0.0171     | '<bos>', '```', '
'
L24      | 3.3008     | 2.4947     | 3.0556     | 0.0148     | '.', ' answer', '"}'
L25      | 3.4200     | 2.6344     | 2.9913     | 0.0126     | '
', '.', ' answer'
L26      | 3.4326     | 2.6056     | 3.1309     | 0.0151     | '<bos>', '```', '.'
L27      | 3.1838     | 2.5451     | 2.8371     | 0.0160     | '.', ':', ' ...)'
L28      | 3.2193     | 2.5267     | 2.9951     | 0.0136     | ''', '<bos>', 'Respond'
L29      | 2.7086     | 2.1045     | 2.6714     | 0.0132     | '<bos>', '```', 'Respond'
L30      | 2.5825     | 2.1535     | 2.5344     | 0.0169     | '<bos>', '",', '```'
L31      | 2.5471     | 1.8868     | 2.2671     | 0.0219     | '<bos>', '
', '```'
L32      | 2.2845     | 1.6772     | 2.0600     | 0.0242     | '<bos>', ' format', '
'
L33      | 2.4555     | 1.6951     | 2.1234     | 0.0243     | '<bos>', ' letter', 'Respond'
L34      | 2.0308     | 1.4524     | 1.7489     | 0.0262     | '<bos>', ' answer', '.'
L35      | 2.7996     | 1.9160     | 2.5035     | 0.0187     | '<bos>', ' ...)', '```'
L36      | 2.4001     | 1.8344     | 2.1512     | 0.0237     | '.', '<bos>', '```'
L37      | 2.5495     | 1.7609     | 2.3343     | 0.0249     | '<bos>', 'answer', ' is'
L38      | 2.0183     | 1.2543     | 1.6014     | 0.0308     | '.', '<bos>', ' answer'
L39      | 1.7829     | 1.1301     | 1.3806     | 0.0345     | '<bos>', ' "', '.'
L40      | 0.9445     | 0.4468     | 0.7572     | 0.0425     | '<bos>', ' answer', ' correct'
L41      | 1.5835     | 0.8140     | 1.3162     | 0.0237     | '<bos>', '##', '

'
L42      | 1.9505     | 1.1393     | 1.5137     | 0.0359     | '<bos>', ' answer', ' X'
L43      | 0.9886     | 0.7134     | 1.0415     | 0.0406     | '<bos>', ' the', ' correct'
L44      | 1.0144     | 0.7521     | 0.9641     | 0.0396     | '<bos>', '.', ','
L45      | 1.0973     | 0.6706     | 0.9334     | 0.0402     | '<bos>', '.', ' "'
L46      | 0.9820     | 0.7583     | 1.0554     | 0.0396     | '<bos>', ' of', '.'
L47      | 1.9919     | 1.2527     | 2.1345     | 0.0293     | '<bos>', '.', '**'
L48      | 0.7056     | 0.3405     | 0.8138     | 0.0459     | '<bos>', ' correct', ' the'
L49      | 0.6829     | 0.3407     | 0.6683     | 0.0449     | '<bos>', ' answer', ' of'
L50      | 0.5852     | 0.3671     | 0.6473     | 0.0436     | '.', '<bos>', ' answer'
L51      | 0.9172     | 0.5816     | 0.9121     | 0.0406     | '<bos>', ' X', 'X'
L52      | 1.2679     | 0.8777     | 1.1755     | 0.0366     | '<bos>', ' answer', ' of'
L53      | 2.0858     | 1.1890     | 1.8333     | 0.0273     | '
', '<bos>', '-'
L54      | 1.7975     | 1.4270     | 1.7148     | 0.0304     | '<bos>', '```', ' answer'
L55      | 1.9844     | 1.2921     | 1.7112     | 0.0339     | ''', '<bos>', '##'
L56      | 1.4973     | 1.1333     | 1.3723     | 0.0319     | '<bos>', ' answer', ' ('
L57      | 0.7991     | 0.5117     | 0.7342     | 0.0381     | '<bos>', '.', ' C'
L58      | 2.4292     | 1.6107     | 2.0027     | 0.0278     | '.', '<bos>', ','
L59      | 2.2647     | 1.1941     | 1.8199     | 0.0242     | '<bos>', '

', '
'
L60      | 1.4862     | 1.0990     | 1.3801     | 0.0276     | '.', '<bos>', ' ...)'
L61      | 1.4964     | 1.1872     | 1.4030     | 0.0234     | '.', '<bos>', '''
----------------------------------------------------------------------------------------------------------

Overall mean entropy (all tokens): 1.4370
Overall mean entropy (last token): 1.9526
Overall mean entropy (last 16 tokens): 1.8250
Most focused layer (all tokens): L3 (entropy: 0.1855)

Results saved to: /root/CoTLab/CoTLab/outputs/2026-02-26/16-24-28_attention_analysis_medgemma_27b_text_it_mcq_json_mmlu_medical/results.json

Metrics:
  num_samples_analyzed: 644
  num_layers_analyzed: 62
  num_heads: 32
  overall_mean_entropy: 1.4369699381881471
  overall_mean_entropy_all_tokens: 1.4369699381881471
  overall_mean_entropy_last_token: 1.9525581193372525
  overall_mean_entropy_last_k_tokens: 1.825019686544607
  last_k_tokens: 16
  most_focused_layer: 3
  most_focused_entropy: 0.18548793898855298
  most_focused_layer_all_tokens: 3
  most_focused_entropy_all_tokens: 0.18548793898855298
  most_focused_layer_last_token: 3
  most_focused_entropy_last_token: 0.25373178025185295
  analyze_generated_tokens: False

Experiment documentation updated: /root/CoTLab/CoTLab/outputs/2026-02-26/16-24-28_attention_analysis_medgemma_27b_text_it_mcq_json_mmlu_medical/EXPERIMENT.md

Experiment complete.
