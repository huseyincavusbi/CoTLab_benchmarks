============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.pubmedqa.PubMedQAPromptStrategy
  name: pubmedqa
  output_format: json
  few_shot: true
  answer_first: false
  contrarian: false
dataset:
  _target_: cotlab.datasets.loaders.PubMedQADataset
  name: pubmedqa
  filename: pubmedqa/test.jsonl
experiment:
  _target_: cotlab.experiments.AttentionAnalysisExperiment
  name: attention_analysis
  description: Analyze attention patterns at critical layers
  all_layers: true
  target_layers: null
  layer_stride: 1
  force_eager_reload: false
  num_samples: null
  last_k_tokens: 16
  max_input_tokens: 1024
  analyze_generated_tokens: false
  generated_max_new_tokens: 16
  generated_do_sample: false
  generated_temperature: 0.7
  generated_top_p: 0.9
  question: Patient presents with chest pain, sweating, and shortness of breath. What
    is the diagnosis?
  batch_size: 4
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /home/ubuntu/CoTLab/outputs/2026-02-26/17-35-35_attention_analysis_medgemma_27b_text_it_pubmedqa_json_pubmedqa/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:07,  1.42it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.40it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.40it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:05,  1.39it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.42it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.43it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:04<00:02,  1.41it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.41it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.42it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.47it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.43it/s]
  Resolved device: cuda:0
Creating prompt strategy: pubmedqa
Loading dataset: pubmedqa
Creating experiment: attention_analysis
============================================================
Running experiment: attention_analysis
============================================================
Model: google/medgemma-27b-text-it
Attention heads: 32
All layers enabled: True
Layer stride: 1
Resolved layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
Max input tokens: 1024
Batch size: 4
Analyze generated tokens: False
Current attention implementation: sdpa
Switching attention implementation to 'eager' in-place...

Analyzing attention on 500 samples (batch_size=4)...
Processing batches:   0%|                                                 | 0/125 [00:00<?, ?it/s]Processing batches:   1%|▎                                        | 1/125 [00:02<04:54,  2.37s/it]Processing batches:   2%|▋                                        | 2/125 [00:04<04:19,  2.11s/it]Processing batches:   2%|▉                                        | 3/125 [00:06<04:04,  2.00s/it]Processing batches:   3%|█▎                                       | 4/125 [00:08<03:59,  1.98s/it]Processing batches:   4%|█▋                                       | 5/125 [00:10<03:58,  1.99s/it]Processing batches:   5%|█▉                                       | 6/125 [00:12<03:55,  1.98s/it]Processing batches:   6%|██▎                                      | 7/125 [00:14<03:52,  1.97s/it]Processing batches:   6%|██▌                                      | 8/125 [00:16<03:50,  1.97s/it]Processing batches:   7%|██▉                                      | 9/125 [00:17<03:46,  1.96s/it]Processing batches:   8%|███▏                                    | 10/125 [00:19<03:41,  1.93s/it]Processing batches:   9%|███▌                                    | 11/125 [00:21<03:39,  1.92s/it]Processing batches:  10%|███▊                                    | 12/125 [00:23<03:40,  1.95s/it]Processing batches:  10%|████▏                                   | 13/125 [00:25<03:37,  1.94s/it]Processing batches:  11%|████▍                                   | 14/125 [00:27<03:37,  1.96s/it]Processing batches:  12%|████▊                                   | 15/125 [00:29<03:35,  1.96s/it]Processing batches:  13%|█████                                   | 16/125 [00:31<03:35,  1.98s/it]Processing batches:  14%|█████▍                                  | 17/125 [00:33<03:31,  1.96s/it]Processing batches:  14%|█████▊                                  | 18/125 [00:35<03:27,  1.94s/it]Processing batches:  15%|██████                                  | 19/125 [00:37<03:27,  1.96s/it]Processing batches:  16%|██████▍                                 | 20/125 [00:39<03:25,  1.96s/it]Processing batches:  17%|██████▋                                 | 21/125 [00:41<03:25,  1.97s/it]Processing batches:  18%|███████                                 | 22/125 [00:43<03:22,  1.96s/it]Processing batches:  18%|███████▎                                | 23/125 [00:45<03:24,  2.01s/it]Processing batches:  19%|███████▋                                | 24/125 [00:47<03:20,  1.99s/it]Processing batches:  20%|████████                                | 25/125 [00:49<03:18,  1.99s/it]Processing batches:  21%|████████▎                               | 26/125 [00:51<03:16,  1.98s/it]Processing batches:  22%|████████▋                               | 27/125 [00:53<03:13,  1.98s/it]Processing batches:  22%|████████▉                               | 28/125 [00:55<03:13,  1.99s/it]Processing batches:  23%|█████████▎                              | 29/125 [00:57<03:10,  1.99s/it]Processing batches:  24%|█████████▌                              | 30/125 [00:59<03:12,  2.02s/it]Processing batches:  25%|█████████▉                              | 31/125 [01:01<03:08,  2.01s/it]Processing batches:  26%|██████████▏                             | 32/125 [01:03<03:07,  2.02s/it]Processing batches:  26%|██████████▌                             | 33/125 [01:05<03:03,  2.00s/it]Processing batches:  27%|██████████▉                             | 34/125 [01:07<03:01,  1.99s/it]Processing batches:  28%|███████████▏                            | 35/125 [01:09<02:59,  2.00s/it]Processing batches:  29%|███████████▌                            | 36/125 [01:11<02:58,  2.00s/it]Processing batches:  30%|███████████▊                            | 37/125 [01:13<02:55,  2.00s/it]Processing batches:  30%|████████████▏                           | 38/125 [01:15<02:50,  1.96s/it]Processing batches:  31%|████████████▍                           | 39/125 [01:17<02:48,  1.95s/it]Processing batches:  32%|████████████▊                           | 40/125 [01:19<02:48,  1.98s/it]Processing batches:  33%|█████████████                           | 41/125 [01:21<02:45,  1.97s/it]Processing batches:  34%|█████████████▍                          | 42/125 [01:23<02:43,  1.97s/it]Processing batches:  34%|█████████████▊                          | 43/125 [01:25<02:41,  1.97s/it]Processing batches:  35%|██████████████                          | 44/125 [01:27<02:38,  1.96s/it]Processing batches:  36%|██████████████▍                         | 45/125 [01:29<02:39,  1.99s/it]Processing batches:  37%|██████████████▋                         | 46/125 [01:31<02:36,  1.98s/it]Processing batches:  38%|███████████████                         | 47/125 [01:32<02:33,  1.96s/it]Processing batches:  38%|███████████████▎                        | 48/125 [01:34<02:30,  1.95s/it]Processing batches:  39%|███████████████▋                        | 49/125 [01:36<02:31,  1.99s/it]Processing batches:  40%|████████████████                        | 50/125 [01:38<02:28,  1.97s/it]Processing batches:  41%|████████████████▎                       | 51/125 [01:40<02:27,  1.99s/it]Processing batches:  42%|████████████████▋                       | 52/125 [01:42<02:25,  1.99s/it]Processing batches:  42%|████████████████▉                       | 53/125 [01:44<02:22,  1.98s/it]Processing batches:  43%|█████████████████▎                      | 54/125 [01:46<02:20,  1.99s/it]Processing batches:  44%|█████████████████▌                      | 55/125 [01:48<02:19,  1.99s/it]Processing batches:  45%|█████████████████▉                      | 56/125 [01:50<02:19,  2.02s/it]Processing batches:  46%|██████████████████▏                     | 57/125 [01:52<02:15,  1.99s/it]Processing batches:  46%|██████████████████▌                     | 58/125 [01:55<02:15,  2.02s/it]Processing batches:  47%|██████████████████▉                     | 59/125 [01:57<02:13,  2.03s/it]Processing batches:  48%|███████████████████▏                    | 60/125 [01:59<02:10,  2.01s/it]Processing batches:  49%|███████████████████▌                    | 61/125 [02:00<02:06,  1.98s/it]Processing batches:  50%|███████████████████▊                    | 62/125 [02:03<02:06,  2.01s/it]Processing batches:  50%|████████████████████▏                   | 63/125 [02:05<02:04,  2.01s/it]Processing batches:  51%|████████████████████▍                   | 64/125 [02:06<02:01,  1.99s/it]Processing batches:  52%|████████████████████▊                   | 65/125 [02:09<02:03,  2.07s/it]Processing batches:  53%|█████████████████████                   | 66/125 [02:11<01:59,  2.03s/it]Processing batches:  54%|█████████████████████▍                  | 67/125 [02:13<01:57,  2.03s/it]Processing batches:  54%|█████████████████████▊                  | 68/125 [02:15<01:54,  2.01s/it]Processing batches:  55%|██████████████████████                  | 69/125 [02:17<01:50,  1.97s/it]Processing batches:  56%|██████████████████████▍                 | 70/125 [02:19<01:49,  1.99s/it]Processing batches:  57%|██████████████████████▋                 | 71/125 [02:21<01:47,  1.99s/it]Processing batches:  58%|███████████████████████                 | 72/125 [02:22<01:44,  1.98s/it]Processing batches:  58%|███████████████████████▎                | 73/125 [02:24<01:42,  1.97s/it]Processing batches:  59%|███████████████████████▋                | 74/125 [02:26<01:39,  1.96s/it]Processing batches:  60%|████████████████████████                | 75/125 [02:28<01:37,  1.95s/it]Processing batches:  61%|████████████████████████▎               | 76/125 [02:30<01:34,  1.94s/it]Processing batches:  62%|████████████████████████▋               | 77/125 [02:32<01:33,  1.94s/it]Processing batches:  62%|████████████████████████▉               | 78/125 [02:34<01:31,  1.95s/it]Processing batches:  63%|█████████████████████████▎              | 79/125 [02:36<01:28,  1.93s/it]Processing batches:  64%|█████████████████████████▌              | 80/125 [02:38<01:27,  1.94s/it]Processing batches:  65%|█████████████████████████▉              | 81/125 [02:40<01:26,  1.96s/it]Processing batches:  66%|██████████████████████████▏             | 82/125 [02:42<01:23,  1.94s/it]Processing batches:  66%|██████████████████████████▌             | 83/125 [02:44<01:22,  1.96s/it]Processing batches:  67%|██████████████████████████▉             | 84/125 [02:46<01:20,  1.96s/it]Processing batches:  68%|███████████████████████████▏            | 85/125 [02:48<01:18,  1.97s/it]Processing batches:  69%|███████████████████████████▌            | 86/125 [02:50<01:16,  1.96s/it]Processing batches:  70%|███████████████████████████▊            | 87/125 [02:52<01:13,  1.94s/it]Processing batches:  70%|████████████████████████████▏           | 88/125 [02:54<01:11,  1.94s/it]Processing batches:  71%|████████████████████████████▍           | 89/125 [02:56<01:10,  1.95s/it]Processing batches:  72%|████████████████████████████▊           | 90/125 [02:58<01:08,  1.96s/it]Processing batches:  73%|█████████████████████████████           | 91/125 [02:59<01:05,  1.93s/it]Processing batches:  74%|█████████████████████████████▍          | 92/125 [03:01<01:03,  1.93s/it]Processing batches:  74%|█████████████████████████████▊          | 93/125 [03:03<01:02,  1.96s/it]Processing batches:  75%|██████████████████████████████          | 94/125 [03:05<01:00,  1.97s/it]Processing batches:  76%|██████████████████████████████▍         | 95/125 [03:07<00:59,  1.97s/it]Processing batches:  77%|██████████████████████████████▋         | 96/125 [03:09<00:57,  1.98s/it]Processing batches:  78%|███████████████████████████████         | 97/125 [03:11<00:55,  1.98s/it]Processing batches:  78%|███████████████████████████████▎        | 98/125 [03:13<00:54,  2.00s/it]Processing batches:  79%|███████████████████████████████▋        | 99/125 [03:15<00:51,  1.99s/it]Processing batches:  80%|███████████████████████████████▏       | 100/125 [03:17<00:49,  1.98s/it]Processing batches:  81%|███████████████████████████████▌       | 101/125 [03:19<00:48,  2.01s/it]Processing batches:  82%|███████████████████████████████▊       | 102/125 [03:21<00:45,  2.00s/it]Processing batches:  82%|████████████████████████████████▏      | 103/125 [03:23<00:43,  2.00s/it]Processing batches:  83%|████████████████████████████████▍      | 104/125 [03:25<00:41,  2.00s/it]Processing batches:  84%|████████████████████████████████▊      | 105/125 [03:27<00:39,  1.98s/it]Processing batches:  85%|█████████████████████████████████      | 106/125 [03:29<00:37,  1.97s/it]Processing batches:  86%|█████████████████████████████████▍     | 107/125 [03:31<00:35,  1.99s/it]Processing batches:  86%|█████████████████████████████████▋     | 108/125 [03:33<00:33,  1.96s/it]Processing batches:  87%|██████████████████████████████████     | 109/125 [03:35<00:31,  1.98s/it]Processing batches:  88%|██████████████████████████████████▎    | 110/125 [03:37<00:29,  1.97s/it]Processing batches:  89%|██████████████████████████████████▋    | 111/125 [03:39<00:27,  1.97s/it]Processing batches:  90%|██████████████████████████████████▉    | 112/125 [03:41<00:25,  1.96s/it]Processing batches:  90%|███████████████████████████████████▎   | 113/125 [03:43<00:23,  1.98s/it]Processing batches:  91%|███████████████████████████████████▌   | 114/125 [03:45<00:21,  1.99s/it]Processing batches:  92%|███████████████████████████████████▉   | 115/125 [03:47<00:19,  1.98s/it]Processing batches:  93%|████████████████████████████████████▏  | 116/125 [03:49<00:17,  1.98s/it]Processing batches:  94%|████████████████████████████████████▌  | 117/125 [03:51<00:15,  1.97s/it]Processing batches:  94%|████████████████████████████████████▊  | 118/125 [03:53<00:13,  1.96s/it]Processing batches:  95%|█████████████████████████████████████▏ | 119/125 [03:55<00:11,  2.00s/it]Processing batches:  96%|█████████████████████████████████████▍ | 120/125 [03:57<00:09,  2.00s/it]Processing batches:  97%|█████████████████████████████████████▊ | 121/125 [03:59<00:07,  2.00s/it]Processing batches:  98%|██████████████████████████████████████ | 122/125 [04:01<00:05,  1.99s/it]Processing batches:  98%|██████████████████████████████████████▍| 123/125 [04:03<00:03,  1.97s/it]Processing batches:  99%|██████████████████████████████████████▋| 124/125 [04:05<00:01,  1.97s/it]Processing batches: 100%|███████████████████████████████████████| 125/125 [04:07<00:00,  1.97s/it]Processing batches: 100%|███████████████████████████████████████| 125/125 [04:07<00:00,  1.98s/it]

======================================================================
ATTENTION ANALYSIS: Aggregated Statistics Across Samples
======================================================================
Layer    | LastTok μ  | AllTok μ   | Last16 μ   | AllTok σ   | Top Tokens
----------------------------------------------------------------------------------------------------------
L0       | 3.7706     | 3.3641     | 3.9786     | 0.0603     | '".', ' "', '",'
L1       | 1.9328     | 1.3759     | 1.8423     | 0.0849     | ' ', '<bos>', '''
L2       | 1.7308     | 1.3305     | 1.8593     | 0.1243     | ' ', '<bos>', '''
L3       | 0.5025     | 0.2427     | 0.5081     | 0.0704     | ' ', '<bos>', '

'
L4       | 1.1502     | 0.9480     | 1.1149     | 0.0366     | ' ', '<bos>', '

'
L5       | 1.4290     | 1.0689     | 1.2522     | 0.0343     | ' ', '<bos>', ' in'
L6       | 2.3443     | 1.4325     | 2.0760     | 0.1441     | ' ', '<bos>', '.'
L7       | 1.9171     | 1.4884     | 1.8803     | 0.0705     | 'maybe', '".', ' "'
L8       | 1.9409     | 1.6713     | 1.9602     | 0.0712     | '".', 'maybe', '",'
L9       | 1.9262     | 1.4696     | 1.8059     | 0.0625     | ' ', '<bos>', '".'
L10      | 1.9791     | 1.5693     | 2.0307     | 0.0780     | ' ', '<bos>', '
'
L11      | 1.7739     | 1.4254     | 1.8229     | 0.0539     | ' ', '<bos>', '##'
L12      | 1.5213     | 1.0141     | 1.4277     | 0.0898     | ' ', '<bos>', 'consistent'
L13      | 1.9275     | 1.3067     | 1.7505     | 0.0834     | ' ', '<bos>', ' your'
L14      | 1.9987     | 1.4009     | 1.8674     | 0.0837     | '".', ':', '",'
L15      | 1.9452     | 1.3784     | 1.6827     | 0.0669     | ' ', '<bos>', '".'
L16      | 1.8489     | 1.5328     | 1.7492     | 0.0465     | ' ', '<bos>', '

'
L17      | 2.4998     | 2.0248     | 2.4042     | 0.0572     | ' ', '<bos>', ' one'
L18      | 3.0862     | 2.4198     | 2.8906     | 0.0535     | '".', '",', 'maybe'
L19      | 2.5012     | 1.8597     | 2.2047     | 0.0590     | ' ', '<bos>', 'yes'
L20      | 2.7461     | 2.0779     | 2.5087     | 0.0591     | '".', 'maybe', '",'
L21      | 2.7767     | 2.2311     | 2.5977     | 0.0621     | '".', '/', 'maybe'
L22      | 2.6175     | 2.2428     | 2.4123     | 0.0564     | '".', ':', '",'
L23      | 3.0895     | 2.5556     | 2.8945     | 0.0649     | '<bos>', '".', ' "'
L24      | 3.2580     | 2.5939     | 3.0465     | 0.0522     | '".', '"}', '```'
L25      | 3.2786     | 2.7356     | 3.0673     | 0.0477     | '".', 'maybe', ' context'
L26      | 3.3823     | 2.7628     | 3.1068     | 0.0590     | 'maybe', '".', '<bos>'
L27      | 3.1507     | 2.6982     | 2.9097     | 0.0645     | '".', 'maybe', ' be'
L28      | 3.2747     | 2.6849     | 3.1950     | 0.0612     | '<bos>', ' of', ' '
L29      | 2.7958     | 2.2905     | 2.6358     | 0.0593     | '<bos>', ':', ' '
L30      | 2.6942     | 2.2917     | 2.6756     | 0.0660     | '<bos>', ' MUST', ' '
L31      | 2.6352     | 2.0726     | 2.5181     | 0.0765     | '<bos>', ' ', ' MUST'
L32      | 2.3225     | 1.8961     | 2.2348     | 0.0799     | '<bos>', '
', ' '
L33      | 2.5049     | 1.8872     | 2.2534     | 0.0840     | '<bos>', '/', ' '
L34      | 2.1587     | 1.6460     | 1.9832     | 0.0841     | '<bos>', '".', ' '
L35      | 2.7652     | 2.1611     | 2.6027     | 0.0692     | '<bos>', ' "', ' '
L36      | 2.4729     | 1.9884     | 2.2713     | 0.0714     | '<bos>', '".', ' '
L37      | 2.8131     | 1.9140     | 2.3738     | 0.0768     | '<bos>', ':', ' '
L38      | 1.9797     | 1.4403     | 1.7213     | 0.0919     | '<bos>', ' ', '
'
L39      | 1.9610     | 1.3221     | 1.6907     | 0.0967     | '<bos>', ' ', '".'
L40      | 1.2058     | 0.6645     | 1.0935     | 0.1259     | '<bos>', ' ', '",'
L41      | 1.8319     | 1.0259     | 1.4215     | 0.0746     | '<bos>', ' ', '##'
L42      | 2.0663     | 1.3775     | 1.7273     | 0.0978     | '<bos>', ' ', 'no'
L43      | 1.2633     | 0.8622     | 1.2286     | 0.1231     | '<bos>', ' ', 'maybe'
L44      | 1.3673     | 0.9449     | 1.3105     | 0.1171     | '<bos>', ' ', '".'
L45      | 1.4866     | 0.8710     | 1.2603     | 0.1170     | '<bos>', ' ', '",'
L46      | 1.5203     | 0.9693     | 1.3716     | 0.1162     | '<bos>', ' ', 'no'
L47      | 1.9814     | 1.6333     | 2.1275     | 0.0799     | '<bos>', '".', ' '
L48      | 1.1567     | 0.5441     | 0.9722     | 0.1407     | '<bos>', ' ', 'answer'
L49      | 1.0515     | 0.5229     | 0.9404     | 0.1359     | '<bos>', ' ', ' "'
L50      | 1.0102     | 0.5415     | 0.9283     | 0.1340     | '<bos>', '
', ' '
L51      | 1.3085     | 0.7740     | 1.1523     | 0.1230     | '<bos>', ' ', ' be'
L52      | 1.5671     | 1.0430     | 1.4017     | 0.1121     | '<bos>', ' ', 'yes'
L53      | 2.0810     | 1.5316     | 1.9436     | 0.0878     | '<bos>', '

', ' '
L54      | 2.1147     | 1.5839     | 1.9213     | 0.0949     | '<bos>', '```', ' '
L55      | 2.2578     | 1.5401     | 1.9669     | 0.1033     | ' ', '<bos>', '##'
L56      | 1.8097     | 1.3041     | 1.5685     | 0.0997     | '<bos>', 'maybe', ' '
L57      | 1.0377     | 0.6749     | 0.9977     | 0.1232     | '<bos>', ' ', 'maybe'
L58      | 2.5292     | 1.8165     | 2.1198     | 0.0918     | '<bos>', '".', ' '
L59      | 2.3614     | 1.5283     | 1.9138     | 0.0985     | '<bos>', '

', ' '
L60      | 1.7521     | 1.1943     | 1.5345     | 0.1010     | '<bos>', '".', ' '
L61      | 1.7831     | 1.2495     | 1.4731     | 0.0850     | '<bos>', '".', ' '
----------------------------------------------------------------------------------------------------------

Overall mean entropy (all tokens): 1.5809
Overall mean entropy (last token): 2.1120
Overall mean entropy (last 16 tokens): 1.9546
Most focused layer (all tokens): L3 (entropy: 0.2427)

Results saved to: /home/ubuntu/CoTLab/outputs/2026-02-26/17-35-35_attention_analysis_medgemma_27b_text_it_pubmedqa_json_pubmedqa/results.json

Metrics:
  num_samples_analyzed: 500
  num_layers_analyzed: 62
  num_heads: 32
  overall_mean_entropy: 1.5808691877125216
  overall_mean_entropy_all_tokens: 1.5808691877125216
  overall_mean_entropy_last_token: 2.112046961684907
  overall_mean_entropy_last_k_tokens: 1.9545540839273021
  last_k_tokens: 16
  most_focused_layer: 3
  most_focused_entropy: 0.24273190930938654
  most_focused_layer_all_tokens: 3
  most_focused_entropy_all_tokens: 0.24273190930938654
  most_focused_layer_last_token: 3
  most_focused_entropy_last_token: 0.502548228406114
  analyze_generated_tokens: False

Experiment documentation updated: /home/ubuntu/CoTLab/outputs/2026-02-26/17-35-35_attention_analysis_medgemma_27b_text_it_pubmedqa_json_pubmedqa/EXPERIMENT.md

Experiment complete.
