============================================================
Configuration:
============================================================
backend:
  _target_: cotlab.backends.TransformersBackend
  device: cuda
  dtype: bfloat16
  enable_hooks: true
  trust_remote_code: true
model:
  name: google/medgemma-27b-text-it
  variant: 27b-text
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  safe_name: medgemma_27b_text_it
prompt:
  _target_: cotlab.prompts.mcq.MCQPromptStrategy
  name: mcq
  few_shot: true
  output_format: json
  answer_first: false
  contrarian: false
dataset:
  _target_: cotlab.datasets.loaders.MedQADataset
  name: medqa
  filename: medqa/test.jsonl
  split: test
experiment:
  _target_: cotlab.experiments.AttentionAnalysisExperiment
  name: attention_analysis
  description: Analyze attention patterns at critical layers
  all_layers: true
  target_layers: null
  layer_stride: 1
  force_eager_reload: false
  num_samples: null
  last_k_tokens: 16
  max_input_tokens: 1024
  analyze_generated_tokens: false
  generated_max_new_tokens: 16
  generated_do_sample: false
  generated_temperature: 0.7
  generated_top_p: 0.9
  question: Patient presents with chest pain, sweating, and shortness of breath. What
    is the diagnosis?
  batch_size: 16
seed: 42
verbose: true
dry_run: false

============================================================
Created experiment documentation: /root/CoTLab/CoTLab/outputs/2026-02-26/16-14-59_attention_analysis_medgemma_27b_text_it_mcq_json_medqa/EXPERIMENT.md
Loading backend: cotlab.backends.TransformersBackend
Loading model: google/medgemma-27b-text-it
  Device map: cuda
  Dtype: torch.bfloat16
  Cache: ~/.cache/huggingface (HF default)
Loading checkpoint shards:   0%|                                           | 0/11 [00:00<?, ?it/s]Loading checkpoint shards:   9%|███▏                               | 1/11 [00:00<00:06,  1.47it/s]Loading checkpoint shards:  18%|██████▎                            | 2/11 [00:01<00:06,  1.44it/s]Loading checkpoint shards:  27%|█████████▌                         | 3/11 [00:02<00:05,  1.43it/s]Loading checkpoint shards:  36%|████████████▋                      | 4/11 [00:02<00:04,  1.43it/s]Loading checkpoint shards:  45%|███████████████▉                   | 5/11 [00:03<00:04,  1.43it/s]Loading checkpoint shards:  55%|███████████████████                | 6/11 [00:04<00:03,  1.42it/s]Loading checkpoint shards:  64%|██████████████████████▎            | 7/11 [00:04<00:02,  1.42it/s]Loading checkpoint shards:  73%|█████████████████████████▍         | 8/11 [00:05<00:02,  1.42it/s]Loading checkpoint shards:  82%|████████████████████████████▋      | 9/11 [00:06<00:01,  1.42it/s]Loading checkpoint shards:  91%|██████████████████████████████▉   | 10/11 [00:07<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████████████████████████████| 11/11 [00:07<00:00,  1.43it/s]
  Resolved device: cuda:0
Creating prompt strategy: mcq
Loading dataset: medqa
test.jsonl: 0.00B [00:00, ?B/s]test.jsonl: 1.94MB [00:00, 19.4MB/s]test.jsonl: 2.08MB [00:00, 20.2MB/s]
Creating experiment: attention_analysis
============================================================
Running experiment: attention_analysis
============================================================
Model: google/medgemma-27b-text-it
Attention heads: 32
All layers enabled: True
Layer stride: 1
Resolved layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]
Max input tokens: 1024
Batch size: 16
Analyze generated tokens: False
Current attention implementation: sdpa
Switching attention implementation to 'eager' in-place...

Analyzing attention on 1273 samples (batch_size=16)...
Processing batches:   0%|                                                  | 0/80 [00:00<?, ?it/s]Processing batches:   1%|▌                                         | 1/80 [00:07<09:15,  7.03s/it]Processing batches:   2%|█                                         | 2/80 [00:13<08:54,  6.86s/it]Processing batches:   4%|█▌                                        | 3/80 [00:20<08:51,  6.90s/it]Processing batches:   5%|██                                        | 4/80 [00:27<08:42,  6.88s/it]Processing batches:   6%|██▋                                       | 5/80 [00:34<08:38,  6.91s/it]Processing batches:   8%|███▏                                      | 6/80 [00:41<08:30,  6.90s/it]Processing batches:   9%|███▋                                      | 7/80 [00:48<08:23,  6.90s/it]Processing batches:  10%|████▏                                     | 8/80 [00:54<08:11,  6.83s/it]Processing batches:  11%|████▋                                     | 9/80 [01:01<08:06,  6.86s/it]Processing batches:  12%|█████▏                                   | 10/80 [01:09<08:05,  6.94s/it]Processing batches:  14%|█████▋                                   | 11/80 [01:15<07:50,  6.82s/it]Processing batches:  15%|██████▏                                  | 12/80 [01:22<07:45,  6.84s/it]Processing batches:  16%|██████▋                                  | 13/80 [01:28<07:31,  6.74s/it]Processing batches:  18%|███████▏                                 | 14/80 [01:35<07:28,  6.79s/it]Processing batches:  19%|███████▋                                 | 15/80 [01:42<07:24,  6.84s/it]Processing batches:  20%|████████▏                                | 16/80 [01:49<07:15,  6.81s/it]Processing batches:  21%|████████▋                                | 17/80 [01:56<07:11,  6.84s/it]Processing batches:  22%|█████████▏                               | 18/80 [02:03<07:06,  6.88s/it]Processing batches:  24%|█████████▋                               | 19/80 [02:10<07:03,  6.94s/it]Processing batches:  25%|██████████▎                              | 20/80 [02:17<06:50,  6.85s/it]Processing batches:  26%|██████████▊                              | 21/80 [02:24<06:43,  6.84s/it]Processing batches:  28%|███████████▎                             | 22/80 [02:30<06:36,  6.83s/it]Processing batches:  29%|███████████▊                             | 23/80 [02:37<06:31,  6.87s/it]Processing batches:  30%|████████████▎                            | 24/80 [02:44<06:26,  6.90s/it]Processing batches:  31%|████████████▊                            | 25/80 [02:51<06:21,  6.93s/it]Processing batches:  32%|█████████████▎                           | 26/80 [02:58<06:15,  6.95s/it]Processing batches:  34%|█████████████▊                           | 27/80 [03:05<06:09,  6.96s/it]Processing batches:  35%|██████████████▎                          | 28/80 [03:12<05:58,  6.90s/it]Processing batches:  36%|██████████████▊                          | 29/80 [03:19<05:51,  6.90s/it]Processing batches:  38%|███████████████▍                         | 30/80 [03:26<05:41,  6.84s/it]Processing batches:  39%|███████████████▉                         | 31/80 [03:32<05:32,  6.79s/it]Processing batches:  40%|████████████████▍                        | 32/80 [03:39<05:25,  6.79s/it]Processing batches:  41%|████████████████▉                        | 33/80 [03:46<05:24,  6.90s/it]Processing batches:  42%|█████████████████▍                       | 34/80 [03:53<05:20,  6.96s/it]Processing batches:  44%|█████████████████▉                       | 35/80 [04:00<05:13,  6.97s/it]Processing batches:  45%|██████████████████▍                      | 36/80 [04:07<05:05,  6.94s/it]Processing batches:  46%|██████████████████▉                      | 37/80 [04:14<04:58,  6.95s/it]Processing batches:  48%|███████████████████▍                     | 38/80 [04:21<04:48,  6.88s/it]Processing batches:  49%|███████████████████▉                     | 39/80 [04:28<04:42,  6.89s/it]Processing batches:  50%|████████████████████▌                    | 40/80 [04:35<04:36,  6.91s/it]Processing batches:  51%|█████████████████████                    | 41/80 [04:42<04:28,  6.89s/it]Processing batches:  52%|█████████████████████▌                   | 42/80 [04:49<04:23,  6.94s/it]Processing batches:  54%|██████████████████████                   | 43/80 [04:55<04:15,  6.90s/it]Processing batches:  55%|██████████████████████▌                  | 44/80 [05:02<04:08,  6.89s/it]Processing batches:  56%|███████████████████████                  | 45/80 [05:09<04:03,  6.96s/it]Processing batches:  57%|███████████████████████▌                 | 46/80 [05:16<03:55,  6.91s/it]Processing batches:  59%|████████████████████████                 | 47/80 [05:23<03:48,  6.92s/it]Processing batches:  60%|████████████████████████▌                | 48/80 [05:30<03:41,  6.93s/it]Processing batches:  61%|█████████████████████████                | 49/80 [05:37<03:35,  6.94s/it]Processing batches:  62%|█████████████████████████▋               | 50/80 [05:44<03:25,  6.85s/it]Processing batches:  64%|██████████████████████████▏              | 51/80 [05:50<03:16,  6.79s/it]Processing batches:  65%|██████████████████████████▋              | 52/80 [05:57<03:11,  6.84s/it]Processing batches:  66%|███████████████████████████▏             | 53/80 [06:04<03:02,  6.77s/it]Processing batches:  68%|███████████████████████████▋             | 54/80 [06:11<02:58,  6.86s/it]Processing batches:  69%|████████████████████████████▏            | 55/80 [06:18<02:51,  6.87s/it]Processing batches:  70%|████████████████████████████▋            | 56/80 [06:25<02:45,  6.88s/it]Processing batches:  71%|█████████████████████████████▏           | 57/80 [06:32<02:39,  6.94s/it]Processing batches:  72%|█████████████████████████████▋           | 58/80 [06:39<02:31,  6.90s/it]Processing batches:  74%|██████████████████████████████▏          | 59/80 [06:46<02:24,  6.90s/it]Processing batches:  75%|██████████████████████████████▊          | 60/80 [06:53<02:18,  6.94s/it]Processing batches:  76%|███████████████████████████████▎         | 61/80 [07:00<02:12,  6.97s/it]Processing batches:  78%|███████████████████████████████▊         | 62/80 [07:06<02:04,  6.89s/it]Processing batches:  79%|████████████████████████████████▎        | 63/80 [07:13<01:56,  6.86s/it]Processing batches:  80%|████████████████████████████████▊        | 64/80 [07:20<01:49,  6.86s/it]Processing batches:  81%|█████████████████████████████████▎       | 65/80 [07:27<01:42,  6.84s/it]Processing batches:  82%|█████████████████████████████████▊       | 66/80 [07:34<01:35,  6.82s/it]Processing batches:  84%|██████████████████████████████████▎      | 67/80 [07:40<01:28,  6.84s/it]Processing batches:  85%|██████████████████████████████████▊      | 68/80 [07:47<01:21,  6.80s/it]Processing batches:  86%|███████████████████████████████████▎     | 69/80 [07:54<01:14,  6.79s/it]Processing batches:  88%|███████████████████████████████████▉     | 70/80 [08:01<01:08,  6.89s/it]Processing batches:  89%|████████████████████████████████████▍    | 71/80 [08:08<01:02,  6.95s/it]Processing batches:  90%|████████████████████████████████████▉    | 72/80 [08:15<00:54,  6.86s/it]Processing batches:  91%|█████████████████████████████████████▍   | 73/80 [08:22<00:48,  6.95s/it]Processing batches:  92%|█████████████████████████████████████▉   | 74/80 [08:29<00:41,  6.99s/it]Processing batches:  94%|██████████████████████████████████████▍  | 75/80 [08:36<00:34,  6.88s/it]Processing batches:  95%|██████████████████████████████████████▉  | 76/80 [08:42<00:27,  6.87s/it]Processing batches:  96%|███████████████████████████████████████▍ | 77/80 [08:50<00:21,  7.03s/it]Processing batches:  98%|███████████████████████████████████████▉ | 78/80 [08:57<00:14,  7.06s/it]Processing batches:  99%|████████████████████████████████████████▍| 79/80 [09:04<00:06,  6.98s/it]Processing batches: 100%|█████████████████████████████████████████| 80/80 [09:08<00:00,  6.04s/it]Processing batches: 100%|█████████████████████████████████████████| 80/80 [09:08<00:00,  6.85s/it]

======================================================================
ATTENTION ANALYSIS: Aggregated Statistics Across Samples
======================================================================
Layer    | LastTok μ  | AllTok μ   | Last16 μ   | AllTok σ   | Top Tokens
----------------------------------------------------------------------------------------------------------
L0       | 3.6843     | 3.3861     | 4.1511     | 0.0613     | '.', ',', ' of'
L1       | 1.6285     | 1.4318     | 1.7822     | 0.0766     | ' ', '<bos>', '.'
L2       | 1.6630     | 1.3954     | 1.9733     | 0.1084     | ' ', '<bos>', '.'
L3       | 0.5077     | 0.2732     | 0.5054     | 0.0589     | ' ', '<bos>', '.'
L4       | 1.0975     | 0.9418     | 1.1804     | 0.0304     | ' ', '<bos>', '.'
L5       | 1.4819     | 1.0837     | 1.3612     | 0.0302     | ' ', '<bos>', ' in'
L6       | 1.9630     | 1.4943     | 2.0335     | 0.1121     | ' ', '<bos>', ' ...)'
L7       | 2.0411     | 1.5324     | 1.9639     | 0.0687     | ' ', '<bos>', ' correct'
L8       | 2.2221     | 1.7102     | 2.0958     | 0.0753     | '.', ' answer', ' '
L9       | 1.9076     | 1.5537     | 2.0254     | 0.0707     | '.', ' answer', '

'
L10      | 2.2770     | 1.6207     | 2.3361     | 0.0660     | ''', '<bos>', ' '
L11      | 1.6295     | 1.4534     | 1.8975     | 0.0454     | ''', '<bos>', ' '
L12      | 1.7903     | 1.2145     | 1.8121     | 0.0822     | ' ', '<bos>', '''
L13      | 2.2944     | 1.4887     | 2.0944     | 0.0786     | ' ', '<bos>', '''
L14      | 2.2251     | 1.5626     | 2.0231     | 0.0786     | ':', '.', '
'
L15      | 2.2857     | 1.4724     | 1.9865     | 0.0564     | ''', '<bos>', ' '
L16      | 1.9816     | 1.5941     | 1.9452     | 0.0464     | ' ', '<bos>', '''
L17      | 2.6246     | 2.0140     | 2.5321     | 0.0477     | '<bos>', ' ', '''
L18      | 3.1668     | 2.5219     | 3.1505     | 0.0584     | ',', '.', ' answer'
L19      | 2.6231     | 1.9466     | 2.4811     | 0.0483     | '<bos>', ''', ' '
L20      | 2.8708     | 2.0917     | 2.7083     | 0.0438     | '.', ' answer', ' D'
L21      | 2.8885     | 2.2893     | 2.7342     | 0.0552     | '.', ' answer', '```'
L22      | 3.1024     | 2.2832     | 2.7655     | 0.0470     | '.', ' answer', ' of'
L23      | 3.0828     | 2.5329     | 2.9509     | 0.0575     | '<bos>', '```', ' '
L24      | 3.3983     | 2.6047     | 3.1798     | 0.0445     | '.', ' answer', '"}'
L25      | 3.4739     | 2.7233     | 3.1209     | 0.0379     | '
', '.', ' answer'
L26      | 3.4989     | 2.7100     | 3.2601     | 0.0445     | '.', '<bos>', '```'
L27      | 3.2874     | 2.6819     | 3.0833     | 0.0605     | '.', ':', ' ...)'
L28      | 3.3620     | 2.6394     | 3.1966     | 0.0489     | ' ', '<bos>', '''
L29      | 2.7289     | 2.1895     | 2.7823     | 0.0413     | '<bos>', '```', ' '
L30      | 2.8244     | 2.2635     | 2.7763     | 0.0506     | '<bos>', ' ', '",'
L31      | 2.8577     | 2.0194     | 2.5795     | 0.0556     | '<bos>', ' ', '
'
L32      | 2.5762     | 1.8253     | 2.3815     | 0.0619     | '<bos>', ' ', ' format'
L33      | 2.7642     | 1.8450     | 2.4278     | 0.0662     | '<bos>', ' ', ' letter'
L34      | 2.3159     | 1.6050     | 2.0606     | 0.0667     | '<bos>', ' ', ' answer'
L35      | 2.9174     | 2.0229     | 2.6717     | 0.0515     | '<bos>', ' ', ' ...)'
L36      | 2.6522     | 1.9642     | 2.4163     | 0.0515     | '<bos>', ' ', '.'
L37      | 2.7745     | 1.8881     | 2.6436     | 0.0506     | '<bos>', ' ', 'answer'
L38      | 2.3285     | 1.4170     | 1.9492     | 0.0665     | '.', '<bos>', ' '
L39      | 2.1275     | 1.2970     | 1.7764     | 0.0662     | '<bos>', ' ', ' "'
L40      | 1.3993     | 0.6534     | 1.2437     | 0.0861     | '<bos>', ' ', ' answer'
L41      | 1.8660     | 0.9263     | 1.5695     | 0.0504     | '<bos>', ' ', '##'
L42      | 2.2981     | 1.3127     | 1.9272     | 0.0695     | '<bos>', ' ', 'json'
L43      | 1.4531     | 0.9225     | 1.4828     | 0.0843     | '<bos>', ' ', ' the'
L44      | 1.5027     | 0.9589     | 1.4230     | 0.0818     | '<bos>', ' ', '.'
L45      | 1.5803     | 0.8761     | 1.4139     | 0.0823     | '<bos>', ' ', ' "'
L46      | 1.4517     | 0.9541     | 1.5030     | 0.0787     | '<bos>', ' ', ' of'
L47      | 2.2442     | 1.3486     | 2.3696     | 0.0516     | '<bos>', ' ', '.'
L48      | 1.2194     | 0.5772     | 1.3147     | 0.0963     | '<bos>', ' ', ' correct'
L49      | 1.1977     | 0.5737     | 1.1747     | 0.0931     | '<bos>', ' ', ' correct'
L50      | 1.0725     | 0.5909     | 1.1473     | 0.0900     | '<bos>', ' ', '.'
L51      | 1.3991     | 0.7959     | 1.4100     | 0.0866     | '<bos>', ' ', ' C'
L52      | 1.6687     | 1.0629     | 1.5729     | 0.0756     | '<bos>', ' ', '.'
L53      | 2.3045     | 1.3104     | 2.1098     | 0.0568     | '<bos>', ' ', '
'
L54      | 2.1344     | 1.5969     | 2.0580     | 0.0667     | '<bos>', ' ', '```'
L55      | 2.3286     | 1.4655     | 2.1125     | 0.0701     | '<bos>', ' ', '''
L56      | 1.8264     | 1.2994     | 1.7302     | 0.0690     | '<bos>', ' ', '.'
L57      | 1.2410     | 0.7018     | 1.1827     | 0.0835     | '<bos>', ' ', '.'
L58      | 2.6517     | 1.7646     | 2.2741     | 0.0663     | '.', '<bos>', ' '
L59      | 2.4078     | 1.2979     | 2.0181     | 0.0636     | '<bos>', ' ', '

'
L60      | 1.7766     | 1.2215     | 1.6772     | 0.0627     | '<bos>', '.', ' '
L61      | 1.8058     | 1.2845     | 1.6614     | 0.0505     | '<bos>', '.', ' '
----------------------------------------------------------------------------------------------------------

Overall mean entropy (all tokens): 1.5819
Overall mean entropy (last token): 2.2219
Overall mean entropy (last 16 tokens): 2.1157
Most focused layer (all tokens): L3 (entropy: 0.2732)

Results saved to: /root/CoTLab/CoTLab/outputs/2026-02-26/16-14-59_attention_analysis_medgemma_27b_text_it_mcq_json_medqa/results.json

Metrics:
  num_samples_analyzed: 1273
  num_layers_analyzed: 62
  num_heads: 32
  overall_mean_entropy: 1.5819453201969547
  overall_mean_entropy_all_tokens: 1.5819453201969547
  overall_mean_entropy_last_token: 2.2218841725513028
  overall_mean_entropy_last_k_tokens: 2.1156769446355974
  last_k_tokens: 16
  most_focused_layer: 3
  most_focused_entropy: 0.2732310362423496
  most_focused_layer_all_tokens: 3
  most_focused_entropy_all_tokens: 0.2732310362423496
  most_focused_layer_last_token: 3
  most_focused_entropy_last_token: 0.5077367342539146
  analyze_generated_tokens: False

Experiment documentation updated: /root/CoTLab/CoTLab/outputs/2026-02-26/16-14-59_attention_analysis_medgemma_27b_text_it_mcq_json_medqa/EXPERIMENT.md

Experiment complete.
