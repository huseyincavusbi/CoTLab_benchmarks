# Mechanistic Interpretability of MedGemma-27B: OOD vs Non-OOD Circuit Analysis (27.02)

## 1) Sprint Goal

**Question:**  
How do internal circuits (attention patterns, residual-stream representations, and causal layer effects) inside MedGemma-27B differ when the model processes OOD vs non-OOD medical data — and do few-shot examples and introspective prompting engage different circuit pathways?

**Bottom line:**  
OOD data produces **more focused attention** (lower entropy) but **substantially weaker decision-layer representations** compared to non-OOD. Few-shot examples have **near-zero effect on OOD** (mean effect −0.005) but **modestly help on non-OOD** (+0.077). Introspective prompting ("think deeply") **helps universally** across both groups, but benefits non-OOD more (+0.368 vs +0.233). The few-shot signal routes through **L0 → L20 → L32**, while introspection activates a distinct **L20–28 reasoning band**.

**Setup:**  
Ran three mechanistic experiment types on **MedGemma-27B-text-it**:
- **Attention analysis**: 12 completed runs (all layers, stride=1)
- **Activation compare**: 12 completed runs (all layers, stride=1, pooling=last_token)
- **Activation patching**: 20 completed runs (stride=2, 31 layers patched, n=50 per run)
  - 8 runs in `few_shot_contrast` mode (Q2: does few-shot shift representations?)
  - 12 runs in `introspect_contrast` mode (Q3: does "think deeply" engage different circuits?)

All runs stored in [`outputs/2026-02-26/`](outputs/2026-02-26/).  

**Three research questions:**

| ID | Question | Method |
|----|----------|--------|
| Q1 | Do internal activations/circuits change on OOD inference? | Attention analysis + activation compare |
| Q2 | Do few-shot examples shift representations vs zero-shot? | Activation patching (`few_shot_contrast`) |
| Q3 | Does asking the model to "think deeply" change circuits? | Activation patching (`introspect_contrast`) |

**Simple explanation of each experiment:**
- **Attention analysis** measures how spread out or focused the model's attention is at each layer. Low entropy = the model is paying attention to specific tokens. High entropy = attention is diffuse/unfocused.
- **Activation compare** collects the size (norm) of residual-stream vectors at each layer. Larger norms in the final layers = the model is building a stronger "decision signal."
- **Activation patching** is a causal intervention: we run the model twice (e.g. with and without few-shot examples), then swap activations from one run into the other, layer by layer. If swapping a layer changes the output, that layer is causally important for the effect of few-shot (or introspection).

---

## 2) Datasets

**Question:**  
What datasets were covered and which are OOD vs non-OOD?

**Bottom line:**  
12 datasets were tested across 3 experiment types. The 5 OOD datasets are benchmarks the model has never seen during any stage of training. The 7 non-OOD datasets are part of MedGemma's training distribution. 

**OOD datasets**:

| Dataset | N (samples) | Description |
|---------|-------------|-------------|
| `m_arc` | 100 | Medical ARC clinical challenge |
| `medbullets` | 308 | Clinical bullet-point cases |
| `plab` | 1,652 | UK PLAB licensing exam |
| `pubhealthbench` | 760 | UK public health information QA |
| `medxpertqa` | 2,450 | Expert-level medical QA |

**Non-OOD datasets**:

| Dataset | N (samples) | Description |
|---------|-------------|-------------|
| `afrimedqa` | 3,958 | Pan-African multi-specialty QA |
| `medmcqa` | 4,183 | Indian medical PG entrance exam |
| `medqa` | 1,273 | USMLE-style medical QA |
| `mmlu_medical` | 644 | MMLU medical subset |
| `pubmedqa` | 500 | PubMed-based yes/no/maybe QA |
| `histopathology` | 600 | Histopathology diagnosis cases |
| `radiology` | 100 | Radiology diagnosis cases |

---

## 3) General OOD vs Non-OOD Findings

### 3a) Attention Analysis — OOD vs Non-OOD

**Question:**  
Does the model attend differently on OOD vs non-OOD data?

**Bottom line:**  
OOD data produces slightly lower entropy (more focused attention). OOD average mean entropy is **1.470** vs non-OOD **1.534**. Layer 3 is universally the most focused layer across all 12 datasets with no exceptions.

**Key statistics:**

| Metric | OOD avg (5 datasets) | Non-OOD avg (7 datasets) | Delta |
|--------|---------------------|-------------------------|-------|
| Mean entropy (all tokens) | **1.470** | **1.534** | −0.064 |
| Last-token entropy | **1.967** | **2.058** | −0.091 |
| Most focused layer | **3** (all datasets) | **3** (all datasets) | identical |
| Focused-layer entropy | **0.223** | **0.205** | +0.018 |

**Interpretation:**  
The model concentrates attention slightly more tightly on OOD content, but the gap is modest (−0.064). The focused-layer entropy at L3 is actually slightly *higher* for OOD (0.223 vs 0.205), meaning the universal bottleneck layer is marginally less focused on OOD — the overall entropy gap is driven by non-bottleneck layers.

Layer 3 acting as a universal bottleneck (lowest entropy in every dataset) confirms this is a structural property of the 27B architecture, not data-dependent.

**Caveats:**  
- OOD average is pulled down by plab (1.178) and pubhealthbench (1.276), which have particularly structured/short question formats.
- Non-OOD average is pulled down by mmlu_medical (1.437) and medmcqa (1.439).

### 3b) Activation Compare — OOD vs Non-OOD

**Question:**  
Are the model's internal representations stronger or weaker on OOD data?

**Bottom line:**  
**OOD data produces ~10% lower activation norms at the final decision layer** (L61: 34,180 OOD vs 38,113 non-OOD). L58 norms are nearly identical across groups. This means the OOD "decision signal" is substantially weaker in the last few layers.

**Key statistics:**

| Metric | OOD avg (5 datasets) | Non-OOD avg (7 datasets) | Delta |
|--------|---------------------|-------------------------|-------|
| L61 norm | **34,180** | **38,113** | −3,933 (−10.3%) |
| L58 norm | **20,748** | **20,872** | −124 (−0.6%) |
| L57 norm | **16,184** | **16,483** | −299 (−1.8%) |

**Interpretation:**  
The residual stream carries equivalent information through the mid-network (L58 norms are near-identical with <1% gap), but the final decision layer (L61) amplifies the signal **10% less** for OOD content. The model builds a substantially weaker "answer commitment" for data it has never seen during training. This is a stronger effect than the attention entropy gap, suggesting the decision-layer norm is a better discriminator of OOD vs non-OOD than attention focus.

**Caveats:**  
- Pubhealthbench (L61=22,528) and medxpertqa (L61=34,673) pull the OOD average down. Radiology (L61=28,629) pulls the non-OOD average down.

### 3c) Activation Patching — Few-Shot Contrast — OOD vs Non-OOD (Q2)

**Question:**  
Do few-shot examples help or hurt differently on OOD vs non-OOD, and which layers carry the few-shot signal?

**Bottom line:**  
Few-shot has **near-zero effect on OOD** (mean −0.005) and **modestly positive effect on non-OOD** (+0.077, driven by histopathology). The few-shot signal consistently enters at **L0** (embedding) and is processed at **L20** (mid-network). The L16–24 disruption pattern appears on *both* OOD and non-OOD datasets — it is not OOD-specific but rather a general property of few-shot interference.

**Key statistics:**

| Metric | OOD avg (3 datasets) | Non-OOD avg (5 datasets) | Delta |
|--------|---------------------|-------------------------|-------|
| Mean logit recovery | **−0.005** | **+0.077** | +0.082 |
| Peak layer (most common) | **L0** (2/3 datasets) | **L0** (3/5 datasets) | L0 dominant |

*Note: OOD has 3 datasets with few-shot runs (plab and medxpertqa do not support few shot); non-OOD has 5 datasets (afrimedqa, medmcqa, medqa, pubmedqa, histopathology).*

**OOD few-shot detail:**

| Dataset | Mean Effect | Peak L | Peak Effect | Top-5 causal layers |
|---------|-------------|--------|------------|---------------------|
| m_arc | +0.043 | L0 | 1.350 | 0, 20, 32, 36, 12 |
| medbullets | −0.058 | L0 | 1.656 | 0, 20, 32, 38, 12 |
| pubhealthbench | 0.000 | — | 0.000 | (zero effect everywhere) |

**Non-OOD few-shot detail:**

| Dataset | Mean Effect | Peak L | Peak Effect | Top-5 causal layers |
|---------|-------------|--------|------------|---------------------|
| afrimedqa | −0.058 | L0 | 1.784 | 0, 20, 32, 12, 44 |
| medmcqa | −0.081 | L0 | 1.890 | 0, 20, 32, 12, 44 |
| medqa | −0.112 | L0 | 1.407 | 0, 20, 36, 32, 12 |
| pubmedqa | −0.037 | L20 | 1.436 | 20, 58, 56, 54, 0 |
| histopathology | **+0.675** | L26 | 1.200 | 26, 28, 20, 22, 10 |

**Interpretation:**  
Few-shot exemplars cause **negative mid-layer effects (L16–24) on both OOD and non-OOD datasets** — afrimedqa, medmcqa, medqa all show strong negative effects at L16–22 despite being in-distribution. This means the disruption is not about unfamiliarity but about exemplar format interference with the model's existing processing. Histopathology is the dramatic outlier (+0.675) where few-shot massively helps through L26/28 — the model leverages domain knowledge via a specialised pathway. Pubhealthbench shows literally zero effect — few-shot exemplars do not change a single activation for this task.

**Caveats:**  
- Only 8 few-shot contrast runs total (3 OOD, 5 non-OOD); some datasets (plab, mmlu_medical, medxpertqa, radiology) are missing few-shot runs.
- n=50 samples per run; effects may shift with larger samples.

### 3d) Activation Patching — Introspect Contrast — OOD vs Non-OOD (Q3)

**Question:**  
Does the "think deeply" instruction engage different circuits on OOD vs non-OOD data?

**Bottom line:**  
Introspection is **universally positive** — all 12 datasets show positive mean effects. Non-OOD benefits more (**+0.368**) than OOD (**+0.233**). The introspect instruction activates a distinct **L20–28 reasoning band** that is different from the L0-dominated few-shot pathway.

**Key statistics:**

| Metric | OOD avg (5 datasets) | Non-OOD avg (7 datasets) | Delta |
|--------|---------------------|-------------------------|-------|
| Mean logit recovery | **+0.233** | **+0.368** | +0.135 |
| Peak layer (most common) | **L0** or **L24** | **L0** or **L26** | mid-network |

**OOD introspect detail:**

| Dataset | Mean Effect | Peak L | Peak Effect | Top-5 causal layers |
|---------|-------------|--------|------------|---------------------|
| m_arc | +0.287 | L18 | 0.729 | 18, 22, 36, 16, 10 |
| medbullets | +0.391 | L24 | 1.180 | 24, 26, 22, 8, 28 |
| plab | +0.164 | L0 | 1.260 | 0, 10, 20, 6, 12 |
| pubhealthbench | +0.211 | L24 | 0.697 | 24, 22, 34, 32, 8 |
| medxpertqa | +0.114 | L0 | 1.064 | 0, 20, 12, 52, 38 |

**Non-OOD introspect detail:**

| Dataset | Mean Effect | Peak L | Peak Effect | Top-5 causal layers |
|---------|-------------|--------|------------|---------------------|
| afrimedqa | +0.241 | L0 | 0.800 | 0, 20, 32, 12, 56 |
| medmcqa | +0.268 | L24 | 0.500 | 24, 26, 28, 8, 18 |
| medqa | **+0.525** | L26 | 1.281 | 26, 24, 22, 18, 28 |
| mmlu_medical | +0.114 | L0 | 1.120 | 0, 20, 38, 32, 54 |
| pubmedqa | +0.355 | L0 | 0.686 | 0, 8, 40, 24, 26 |
| histopathology | **+0.669** | L26 | 1.935 | 26, 28, 20, 34, 22 |
| radiology | +0.405 | L0 | 1.626 | 0, 24, 22, 28, 50 |

**Interpretation:**  
The "think deeply" instruction benefits every dataset — unlike few-shot, it never hurts. The effect is stronger on non-OOD data, which makes sense: introspection helps the model reason more carefully, and it reasons better about content it already knows. The causal layers cluster in the L20–28 band (especially L24/26), which is distinct from the few-shot L0 → L20 → L32 route. This strongly suggests introspection and few-shot exemplars activate **different circuit pathways** within the model.

Histopathology and medqa are the most responsive to introspection (+0.669 and +0.525), while mmlu_medical and medxpertqa benefit least (+0.114 each). This may reflect task difficulty: introspection helps most on tasks that require genuine reasoning chains, less on tasks that are more about factual recall. Notably, medxpertqa benefits least despite being OOD — suggesting that task difficulty (expert-level questions) rather than OOD status determines introspect responsiveness.

**Caveats:**  
- n=50 samples per run; effects may shift with larger samples.
- The introspect instruction is fixed ("Think deeply about this problem. Carefully reason through…"); different wordings may activate different pathways.

---

## 4) AfriMedQA Results (Non-OOD)

**Question:**  
How does MedGemma-27B process AfriMedQA internally, and what circuits respond to few-shot and introspection?

**Bottom line:**  
AfriMedQA shows moderate attention entropy (1.467), strong L61 norm (38,984), slightly negative few-shot effect (−0.058), and positive introspect effect (+0.241). Despite AfriMedQA being **non-OOD**, few-shot exemplars still slightly disrupt mid-layer processing.

**Setup:**  
- Attention: [`15-15-50_attention_analysis_medgemma_27b_text_it_mcq_json_afrimedqa`](outputs/2026-02-26/15-15-50_attention_analysis_medgemma_27b_text_it_mcq_json_afrimedqa), n=3,958, 62 layers, duration=26m
- Compare: [`17-47-44_activation_compare_medgemma_27b_text_it_mcq_json_afrimedqa`](outputs/2026-02-26/17-47-44_activation_compare_medgemma_27b_text_it_mcq_json_afrimedqa), n=3,958, stride=1, duration=9m
- Patching (few-shot): [`18-33-25_activation_patching_medgemma_27b_text_it_mcq_json_afrimedqa`](outputs/2026-02-26/18-33-25_activation_patching_medgemma_27b_text_it_mcq_json_afrimedqa), n=50, duration=2m
- Patching (introspect): [`19-02-03_activation_patching_medgemma_27b_text_it_mcq_json_afrimedqa`](outputs/2026-02-26/19-02-03_activation_patching_medgemma_27b_text_it_mcq_json_afrimedqa), n=50, duration=4m

**Key findings:**  
- Attention: mean entropy=1.467, last-token=2.021, most focused layer=3 (entropy 0.199)
- Compare: top norm layers L61=38,984 / L58=20,939 / L57=15,595
- Few-shot contrast: mean effect=−0.058, peak at L0 (1.784), top causal layers: 0, 20, 32, 12, 44. Negative at L16 (−0.848), L22 (−0.788), L24 (−0.793).
- Introspect contrast: mean effect=+0.241, peak at L0 (0.800), top causal layers: 0, 20, 32, 12, 56. All layers positive except L60 (−0.028).

**Interpretation:**  
AfriMedQA's strong L61 norm (38,984 — above the non-OOD average of 38,113) confirms the model has solid internal representations for this data, consistent with it being in the training distribution. Few-shot still hurts slightly, following the same mid-layer disruption pattern seen in other training-data benchmarks (medmcqa, medqa). Introspection helps via the generic L0/L20 pathway.

---

## 5) MedMCQA Results (Non-OOD)

**Question:**  
How does MedGemma-27B handle MedMCQA internally?

**Bottom line:**  
MedMCQA has low attention entropy (1.439), strong L61 norm (38,674), negative few-shot effect (−0.081), and moderate introspect benefit (+0.268). Despite being **non-OOD** , few-shot is particularly harmful at layers 16–22.

**Setup:**  
- Attention: [`15-42-21...medmcqa`](outputs/2026-02-26/15-42-21_attention_analysis_medgemma_27b_text_it_mcq_json_medmcqa), n=4,183, duration=28m
- Compare: [`17-57-38...medmcqa`](outputs/2026-02-26/17-57-38_activation_compare_medgemma_27b_text_it_mcq_json_medmcqa), n=4,183, duration=9m
- Patching (few-shot): [`18-36-43...medmcqa`](outputs/2026-02-26/18-36-43_activation_patching_medgemma_27b_text_it_mcq_json_medmcqa), n=50, duration=2m
- Patching (introspect): [`19-06-20...medmcqa`](outputs/2026-02-26/19-06-20_activation_patching_medgemma_27b_text_it_mcq_json_medmcqa), n=50, duration=4m

**Key findings:**  
- Attention: mean entropy=1.439, last-token=1.950, most focused layer=3 (entropy 0.188)
- Compare: top norm layers L61=38,674 / L58=21,125 / L57=15,578
- Few-shot: mean=−0.081, peak L0 (1.890), causal: 0, 20, 32, 12, 44. Strongly negative at L16 (−0.915), L22 (−0.856), L18 (−0.846).
- Introspect: mean=+0.268, peak L24 (0.500), causal: 24, 26, 28, 8, 18. Notably flat effect profile — layers 24/26/28 cluster at 0.500.

**Interpretation:**  
MedMCQA shows the sharpest negative few-shot layer effects in the sprint (L16 at −0.915). This strongly suggests that few-shot disruption is not about unfamiliarity — the model knows this content very well, but exemplars still interfere with mid-layer processing. Introspection distributes benefit evenly across the L24-28 band.

---

## 6) M-ARC Results (OOD — Clinical MCQ Challenge)

**Question:**  
How does MedGemma-27B handle the small but challenging M-ARC dataset?

**Bottom line:**  
M-ARC has high attention entropy (1.609) and low L61 norm (36,744). It's the only OOD dataset where few-shot is mildly positive (+0.043), and introspection works through mid-layers (L18, not L0).

**Setup:**  
- Attention: [`16-09-59...m_arc`](outputs/2026-02-26/16-09-59_attention_analysis_medgemma_27b_text_it_mcq_json_m_arc), n=100, duration=1m
- Compare: [`18-06-28...m_arc`](outputs/2026-02-26/18-06-28_activation_compare_medgemma_27b_text_it_mcq_json_m_arc), n=100, duration=<1m
- Patching (few-shot): [`18-39-00...m_arc`](outputs/2026-02-26/18-39-00_activation_patching_medgemma_27b_text_it_mcq_json_m_arc), n=50, duration=3m
- Patching (introspect): [`19-10-36...m_arc`](outputs/2026-02-26/19-10-36_activation_patching_medgemma_27b_text_it_mcq_json_m_arc), n=50, duration=4.5m

**Key findings:**  
- Attention: mean entropy=1.609, last-token=2.237, most focused layer=3 (entropy 0.283)
- Compare: top norm layers L61=36,744 / L58=19,973 / L57=15,209
- Few-shot: mean=+0.043, peak L0 (1.350), causal: 0, 20, 32, 36, 12. Negative at L18 (−0.484), L22 (−0.420).
- Introspect: mean=+0.287, peak L18 (0.729), causal: 18, 22, 36, 16, 10. Remarkably flat profile — top 6 layers all between 0.61–0.73.

**Interpretation:**  
M-ARC is the only OOD dataset where few-shot is marginally positive — possibly because its MCQ format is close enough to exemplar format. The introspect signal uniquely peaks at L18 instead of L0 or L24, and the effect is very evenly distributed — suggesting M-ARC activates broad reasoning circuits rather than localised ones.

**Caveats:**  
- N=100 is small; attention/compare metrics may be noisy.

---

## 7) MedBullets Results (OOD — Clinical Case Bullets)

**Question:**  
How does MedGemma-27B handle MedBullets internally?

**Bottom line:**  
MedBullets has high attention entropy (1.622), a strong L61 norm (40,065), negative few-shot effect (−0.058), and the strongest OOD introspect benefit (+0.391). The introspect signal is sharply localised at L24–28.

**Setup:**  
- Attention: [`16-12-22...medbullets`](outputs/2026-02-26/16-12-22_attention_analysis_medgemma_27b_text_it_mcq_json_medbullets), n=308, duration=2.5m
- Compare: [`18-07-07...medbullets`](outputs/2026-02-26/18-07-07_activation_compare_medgemma_27b_text_it_mcq_json_medbullets), n=308, duration=1m
- Patching (few-shot): [`18-41-53...medbullets`](outputs/2026-02-26/18-41-53_activation_patching_medgemma_27b_text_it_mcq_json_medbullets), n=50, duration=3m
- Patching (introspect): [`19-15-10...medbullets`](outputs/2026-02-26/19-15-10_activation_patching_medgemma_27b_text_it_mcq_json_medbullets), n=50, duration=4.5m

**Key findings:**  
- Attention: mean entropy=1.622, last-token=2.287, most focused layer=3 (entropy 0.312)
- Compare: top norm layers L61=40,065 / L58=21,165 / L57=16,113
- Few-shot: mean=−0.058, peak L0 (1.656), causal: 0, 20, 32, 38, 12. Negative at L22 (−0.897), L16 (−0.772), L24 (−0.770).
- Introspect: mean=+0.391, peak L24 (1.180), causal: 24, 26, 22, 8, 28. Top 5 layers all above 1.09.

**Interpretation:**  
Despite high L61 norms (strong raw representation), few-shot hurts and introspection strongly helps. This matches a pattern where the model has some underlying capability (high norms indicate confident processing) but needs the right prompting to align that capability with the task. The L24–28 introspect band is very sharp here, with all top-5 layers clustered in a tight range (1.09–1.18).

---

## 8) PLAB Results (OOD — UK Licensing Exam)

**Question:**  
How does MedGemma-27B process PLAB data internally?

**Bottom line:**  
PLAB has the **lowest attention entropy** of any dataset in the sprint (1.178) and moderate L61 norm (36,892). Only introspect-contrast was run — it shows a modest positive effect (+0.164) peaking at L0, with an unusually early causal pathway (L0, L10, L6).

**Setup:**  
- Attention: [`17-15-23...plab`](outputs/2026-02-26/17-15-23_attention_analysis_medgemma_27b_text_it_plab_json_plab), n=1,652, duration=13m
- Compare: [`18-25-06...plab`](outputs/2026-02-26/18-25-06_activation_compare_medgemma_27b_text_it_plab_json_plab), n=1,652, duration=1.5m
- Patching (introspect): [`19-36-37...plab`](outputs/2026-02-26/19-36-37_activation_patching_medgemma_27b_text_it_plab_json_plab), n=50, duration=2m

**Key findings:**  
- Attention: mean entropy=1.178, last-token=1.719, most focused layer=3 (entropy 0.109)
- Compare: top norm layers L61=36,892 / L58=21,666 / L57=17,090
- Introspect: mean=+0.164, peak L0 (1.260), causal: 0, 10, 20, 6, 12. Negative at L2 (−0.271), L30 (−0.268), L34 (−0.268).

**Interpretation:**  
PLAB's extremely low entropy suggests the model treats these questions as highly structured/predictable — possibly because the question format is very regular. The introspect effect is modest and concentrated in early layers (L0/L6/L10), unlike other datasets where L20–28 dominate. 

**Caveats:**  
- No few-shot contrast run available for PLAB.

---

## 9) PubHealthBench Results (OOD — UK Public Health QA)

**Question:**  
How does MedGemma-27B process PubHealthBench internally?

**Bottom line:**  
PubHealthBench is the sprint's most anomalous dataset: low attention entropy (1.276), dramatically low L61 norm (**22,528** — nearly half of other datasets), **zero few-shot effect** across all layers, and moderate introspect benefit (+0.211) concentrated at L24.

**Setup:**  
- Attention: [`17-28-09...pubhealthbench`](outputs/2026-02-26/17-28-09_attention_analysis_medgemma_27b_text_it_pubhealthbench_json_pubhealthbench), n=760, duration=5m
- Compare: [`18-26-35...pubhealthbench`](outputs/2026-02-26/18-26-35_activation_compare_medgemma_27b_text_it_pubhealthbench_json_pubhealthbench), n=760, duration=1m
- Patching (few-shot): [`18-50-46...pubhealthbench`](outputs/2026-02-26/18-50-46_activation_patching_medgemma_27b_text_it_pubhealthbench_json_pubhealthbench), n=50, duration=2m
- Patching (introspect): [`19-39-31...pubhealthbench`](outputs/2026-02-26/19-39-31_activation_patching_medgemma_27b_text_it_pubhealthbench_json_pubhealthbench), n=50, duration=2m

**Key findings:**  
- Attention: mean entropy=1.276, last-token=1.278, most focused layer=3 (entropy 0.103)
- Compare: top norm layers L61=22,528 / L58=21,125 / L57=17,232. **L61 is only barely above L58** — unlike every other dataset where L61 is ~2× L58.
- Few-shot: **mean=0.000, peak=0.000** — literally zero effect at every single layer.
- Introspect: mean=+0.211, peak L24 (0.697), causal: 24, 22, 34, 32, 8. Negative at L58 (−0.155).

**Interpretation:**  
PubHealthBench is structurally different from all other datasets. The near-equal L61/L58 norms suggest the model doesn't build an amplified decision signal in the final layers — it reaches its answer earlier. The zero few-shot effect indicates the task format is so different that exemplars have no pathway to modify activations. Yet introspection still helps through the L24 band, meaning the "think deeply" instruction can still engage reasoning circuits. 

---

## 10) MedQA Results (Non-OOD — USMLE-style QA)

**Question:**  
How does MedGemma-27B handle MedQA internally?

**Bottom line:**  
MedQA has moderate attention entropy (1.582), the second-highest L61 norm in the sprint (40,466), negative few-shot effect (−0.112), and the second-strongest introspect benefit (**+0.525**). The introspect signal is sharply localised at L22–28.

**Setup:**  
- Attention: [`16-14-59...medqa`](outputs/2026-02-26/16-14-59_attention_analysis_medgemma_27b_text_it_mcq_json_medqa), n=1,273, duration=9m
- Compare: [`18-09-02...medqa`](outputs/2026-02-26/18-09-02_activation_compare_medgemma_27b_text_it_mcq_json_medqa), n=1,273, duration=4m
- Patching (few-shot): [`18-45-46...medqa`](outputs/2026-02-26/18-45-46_activation_patching_medgemma_27b_text_it_mcq_json_medqa), n=50, duration=2.5m
- Patching (introspect): [`19-19-44...medqa`](outputs/2026-02-26/19-19-44_activation_patching_medgemma_27b_text_it_mcq_json_medqa), n=50, duration=4m

**Key findings:**  
- Attention: mean entropy=1.582, last-token=2.222, most focused layer=3 (entropy 0.273)
- Compare: top norm layers L61=40,466 / L58=21,050 / L57=15,995
- Few-shot: mean=−0.112, peak L0 (1.407), causal: 0, 20, 36, 32, 12. Strongly negative at L22 (−0.945), L18 (−0.885), L16 (−0.873).
- Introspect: mean=+0.525, peak L26 (1.281), causal: 26, 24, 22, 18, 28. Top 5 layers all between 1.25–1.28.

**Interpretation:**  
MedQA is the highest-responding non-OOD dataset for introspection (+0.525) and the most negatively affected by few-shot (−0.112). Despite being non-OOD, the few-shot signal actively disrupts L16–22 processing. The introspect L22–28 band is extremely tight (1.25–1.28) — nearly uniform activation of the reasoning layers. MedQA is a reasoning-heavy task, which explains why introspection helps so much.

---

## 11) MMLU Medical Results (Non-OOD)

**Question:**  
How does MedGemma-27B handle MMLU medical internally?

**Bottom line:**  
MMLU has low attention entropy (1.437), high L61 norm (39,747), and modest introspect benefit (+0.114) concentrated at L0/L20. No few-shot contrast was run.

**Setup:**  
- Attention: [`16-24-28...mmlu_medical`](outputs/2026-02-26/16-24-28_attention_analysis_medgemma_27b_text_it_mcq_json_mmlu_medical), n=644, duration=4m
- Compare: [`18-12-54...mmlu_medical`](outputs/2026-02-26/18-12-54_activation_compare_medgemma_27b_text_it_mcq_json_mmlu_medical), n=644, duration=1.5m
- Patching (introspect): [`19-25-05...mmlu_medical`](outputs/2026-02-26/19-25-05_activation_patching_medgemma_27b_text_it_mcq_json_mmlu_medical), n=50, duration=4m

**Key findings:**  
- Attention: mean entropy=1.437, last-token=1.953, most focused layer=3 (entropy 0.186)
- Compare: top norm layers L61=39,747 / L58=21,156 / L57=15,501
- Introspect: mean=+0.114, peak L0 (1.120), causal: 0, 20, 38, 32, 54. Negative at L24 (−0.200), L26 (−0.200), L28 (−0.200).

**Interpretation:**  
MMLU is a factual-recall-heavy benchmark — questions often test knowledge rather than reasoning chains. Introspect helps minimally (+0.114), and the L24–28 band that powers introspection on reasoning-heavy tasks is actually **negative** here (all at −0.200). This is the opposite of MedQA/histopathology and suggests introspection actively wastes capacity when the task is about recall, not reasoning.

**Caveats:**  
- No few-shot contrast run available.

---

## 12) MedXpertQA Results (OOD — Expert-Level Medical QA)

**Question:**  
How does MedGemma-27B handle MedXpertQA internally?

**Bottom line:**  
MedXpertQA has the highest attention entropy in the sprint (1.663), a low L61 norm (34,673), and the weakest introspect benefit of any dataset (+0.114). No few-shot contrast was run. MedXpertQA shows hallmarks of genuine OOD processing: diffuse attention, weak decision signals, and minimal responsiveness to prompting interventions.

**Setup:**  
- Attention: [`16-28-57...medxpertqa`](outputs/2026-02-26/16-28-57_attention_analysis_medgemma_27b_text_it_mcq_json_medxpertqa), n=2,450, duration=19m
- Compare: [`18-14-32...medxpertqa`](outputs/2026-02-26/18-14-32_activation_compare_medgemma_27b_text_it_mcq_json_medxpertqa), n=2,450, duration=8m
- Patching (introspect): [`19-29-09...medxpertqa`](outputs/2026-02-26/19-29-09_activation_patching_medgemma_27b_text_it_mcq_json_medxpertqa), n=50, duration=5m

**Key findings:**  
- Attention: mean entropy=1.663, last-token=2.313, most focused layer=3 (entropy 0.310)
- Compare: top norm layers L61=34,673 / L58=19,810 / L57=15,275. High L58 std (86.08) — most variable representations.
- Introspect: mean=+0.114, peak L0 (1.064), causal: 0, 20, 12, 52, 38. Negative at L40 (−0.410), L26 (−0.360), L42 (−0.333).

**Interpretation:**  
MedXpertQA is OOD and expert-level, a double challenge. It shows the highest L58 std (86.08) and highest attention entropy (1.663), meaning the model's internal processing is highly variable and diffuse. The low L61 norm (34,673) is consistent with the OOD group average (34,180) and well below the non-OOD average (38,113). Introspection has only a small effect (+0.114) and routes through L0/L20 (generic pathway) rather than the L24–28 reasoning band — the model lacks deep domain representations to unlock.

**Caveats:**  
- No few-shot contrast run available.

---

## 13) PubMedQA Results (Non-OOD — PubMed Yes/No/Maybe)

**Question:**  
How does MedGemma-27B handle PubMedQA internally?

**Bottom line:**  
PubMedQA has moderate attention entropy (1.581), the highest L61 norm in the sprint (**41,055**), slightly negative few-shot effect (−0.037), and good introspect benefit (+0.355). Uniquely, few-shot peaks at L20 (not L0), and introspect distributes broadly.

**Setup:**  
- Attention: [`17-35-35...pubmedqa`](outputs/2026-02-26/17-35-35_attention_analysis_medgemma_27b_text_it_pubmedqa_json_pubmedqa), n=500, duration=4m
- Compare: [`18-28-03...pubmedqa`](outputs/2026-02-26/18-28-03_activation_compare_medgemma_27b_text_it_pubmedqa_json_pubmedqa), n=500, duration=1.5m
- Patching (few-shot): [`18-54-50...pubmedqa`](outputs/2026-02-26/18-54-50_activation_patching_medgemma_27b_text_it_pubmedqa_json_pubmedqa), n=50, duration=3m
- Patching (introspect): [`19-42-05...pubmedqa`](outputs/2026-02-26/19-42-05_activation_patching_medgemma_27b_text_it_pubmedqa_json_pubmedqa), n=50, duration=4m

**Key findings:**  
- Attention: mean entropy=1.581, last-token=2.112, most focused layer=3 (entropy 0.243)
- Compare: top norm layers L61=41,055 / L58=20,914 / L57=17,479. Lowest L58 std (30.80) — most consistent representations.
- Few-shot: mean=−0.037, peak **L20** (1.436), causal: 20, 58, 56, 54, 0. Unique: L0 is only the 5th most causal layer. Strongly negative at L24 (−0.928).
- Introspect: mean=+0.355, peak L0 (0.686), causal: 0, 8, 40, 24, 26. Very evenly distributed effects (range 0.0–0.69).

**Interpretation:**  
PubMedQA is unique in that few-shot peaks at L20 rather than L0. The few-shot signal enters through the processing layers rather than the embedding. The highest L61 norm in the sprint suggests the model builds very strong decision representations for this task. Introspection helps (+0.355) through a broad, diffuse pathway.

---

## 14) Histopathology Results (Non-OOD — Histopath Diagnosis)

**Question:**  
How does MedGemma-27B handle histopathology internally?

**Bottom line:**  
Histopathology is the **most responsive dataset to both interventions**: few-shot effect (**+0.675**, strongest in the sprint) and introspect effect (**+0.669**, second only by a hair). Both peak at L26 and activate the same L20–28 band. This is the model's "sweet spot". It has strong domain knowledge but needs explicit prompting to fully engage it.

**Setup:**  
- Attention: [`17-39-59...histopathology`](outputs/2026-02-26/17-39-59_attention_analysis_medgemma_27b_text_it_histopathology_json_histopathology), n=600, duration=5m
- Compare: [`18-30-06...histopathology`](outputs/2026-02-26/18-30-06_activation_compare_medgemma_27b_text_it_histopathology_json_histopathology), n=600, duration=2m
- Patching (few-shot): [`18-58-04...histopathology`](outputs/2026-02-26/18-58-04_activation_patching_medgemma_27b_text_it_histopathology_json_histopathology), n=50, duration=3m
- Patching (introspect): [`19-50-05...histopathology`](outputs/2026-02-26/19-50-05_activation_patching_medgemma_27b_text_it_histopathology_json_histopathology), n=50, duration=4m

**Key findings:**  
- Attention: mean entropy=1.650, last-token=2.192, most focused layer=3 (entropy 0.183)
- Compare: top norm layers L61=39,236 / L58=20,446 / L57=18,244. Notably high L57 norm — strongest mid-network signal.
- Few-shot: mean=**+0.675**, peak L26 (1.200), causal: 26, 28, 20, 22, 10. Top 5 layers all between 1.16–1.20 — tight cluster. Negative at L38 (−0.292), L40 (−0.291).
- Introspect: mean=**+0.669**, peak L26 (1.935), causal: 26, 28, 20, 34, 22. Peak is the **highest single-layer effect in the entire sprint** (1.935). Negative at L38 (−0.821), L36 (−0.631).

**Interpretation:**  
Histopathology is the standout dataset: both few-shot and introspection produce massive positive effects through the same L20–28 reasoning band. This is the clearest evidence that the model has deep domain knowledge that needs explicit activation. The nearly identical mean effects (+0.675 vs +0.669) through the same layers suggest both interventions unlock the same underlying capability via the same circuit pathway. The dramatically high single-layer peak (1.935 at L26 for introspect) is nearly double the peak of any other dataset.

---

## 15) Radiology Results (Non-OOD — Radiology Diagnosis)

**Question:**  
How does MedGemma-27B handle radiology internally?

**Bottom line:**  
Radiology has moderate attention entropy (1.585), anomalously low L61 norm (28,629), and strong introspect benefit (+0.405) peaking at L0. No few-shot contrast was run.

**Setup:**  
- Attention: [`17-46-00...radiology`](outputs/2026-02-26/17-46-00_attention_analysis_medgemma_27b_text_it_radiology_json_radiology), n=100, duration=1m
- Compare: [`18-29-38...radiology`](outputs/2026-02-26/18-29-38_activation_compare_medgemma_27b_text_it_radiology_json_radiology), n=100, duration=<1m
- Patching (introspect): [`19-46-37...radiology`](outputs/2026-02-26/19-46-37_activation_patching_medgemma_27b_text_it_radiology_json_radiology), n=50, duration=3m

**Key findings:**  
- Attention: mean entropy=1.585, last-token=1.955, most focused layer=3 (entropy 0.161)
- Compare: top norm layers L61=28,629 / L58=20,477 / L57=16,987. L61 is only 1.4× L58 (vs typical 1.8–2×).
- Introspect: mean=+0.405, peak L0 (1.626), causal: 0, 24, 22, 28, 50. Negative at L6 (−0.331), L52 (−0.295).

**Interpretation:**  
Radiology's low L61 norm (similar to pubhealthbench) suggests the model doesn't amplify decision signals as strongly despite this being non-OOD. Introspection still helps substantially (+0.405) through L0 and the L22–28 band.

**Caveats:**  
- N=100 is small; norms may be noisy.
- No few-shot contrast available.

---

## 16) Answers to Sprint Questions

### Q1 — Do circuits change on OOD inference?

**Answer: Yes, in specific and measurable ways.**
- OOD data produces **slightly more focused attention** (mean entropy 1.470 vs 1.534) — though the gap is modest (−0.064).
- The key discriminator is **decision-layer norms**: OOD L61 norms are **10.3% lower** (34,180 vs 38,113) — the model builds substantially weaker answer representations for truly unseen data.
- The attention focus layer (L3) and overall architecture response are identical — OOD doesn't change **where** the model focuses, but ***how strongly** it commits to its answer.

### Q2 — Do few-shot examples shift representations?

**Answer: Yes, but they shift them in the same negative direction for most datasets.**
- OOD few-shot average: **−0.005** (near-zero effect; only 3 datasets with runs)
- Non-OOD few-shot average: **+0.077** (slightly positive, driven entirely by histopathology +0.675)
- The few-shot signal consistently routes through **L0 → L20 → L32**, and the **mid-layer disruption at L16–24 occurs on both OOD and non-OOD datasets** (afrimedqa, medmcqa, medqa all show strong negative L16–22 despite being in the training data). This is a general few-shot interference pattern, not an OOD-specific effect.
- PubHealthBench shows **literally zero** few-shot effect — the task format is immune to exemplar-based intervention.
- Histopathology is the standout exception (+0.675) where few-shot activates a specialised L26/28 pathway.

### Q3 — Does "think deeply" engage different circuits?

**Answer: Yes, and it always helps.**
- Introspection is positive on **all 12 datasets** (range +0.114 to +0.669).
- It engages a distinct **L20–28 reasoning band** that is different from the few-shot L0 → L20 → L32 pathway.
- Non-OOD benefits more (+0.368 vs +0.233), consistent with the model reasoning better about familiar content.
- Exception: MMLU medical shows **negative** L24–28 effects (−0.200), suggesting introspection is counterproductive for factual-recall tasks.

---

## 17) Key Circuit Findings

| Circuit | Layers | Function | Evidence |
|---------|--------|----------|----------|
| **Attention bottleneck** | L3 | Universal low-entropy gate; structural, not data-dependent | Most focused layer in all 12 datasets |
| **Few-shot entry** | L0 | Embedding layer absorbs few-shot exemplar information | Peak causal layer for few-shot contrast in 7/8 datasets |
| **Few-shot processing** | L20, L32 | Mid-network integration of exemplar signal | Consistently 2nd and 3rd most causal layers |
| **Few-shot disruption** | L16–L24 | Exemplars cause negative effects here (OOD and non-OOD) | Negative patching effects on most datasets with few-shot runs |
| **Introspect reasoning band** | L20–L28 | Core reasoning circuit activated by "think deeply" | Top causal band for introspect in 8/12 datasets |
| **Decision amplification** | L57–L61 | Final representation sharpening → answer commitment | Top norm layers universally; weaker on OOD (−10% at L61) |
