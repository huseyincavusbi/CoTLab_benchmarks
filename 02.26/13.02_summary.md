# OOD Memorization vs Reasoning Summary (13.02)

## 1) Sprint Goal

**Question:**  
Are MedGemma models solving OOD medical datasets via true reasoning, or mostly via benchmark-specific memorization

**Bottom line:**  
Current evidence points to **strong prompt dependence with partial OOD transfer**, not simple memorization.  
With generic MCQ prompting, performance stays low on some sets (M-ARC, MedBullets), but dataset-specific prompts substantially improve results on PubHealthBench and PLAB. This suggests the model has usable capability on OOD tasks, but the capability is highly sensitive to task framing and output constraints.

**Setup:**  
Ran OOD classification multiruns on **MedGemma-27B-text_it** with **vLLM** backend of **CoTLab**, across 4 batches:  
- [`09-23-21`](multirun/2026-02-12/09-23-21) (cross-dataset MCQ matrix)  
- [`12-33-38`](multirun/2026-02-12/12-33-38) (PLAB with MCQ prompt matrix)  
- [`13-49-50`](multirun/2026-02-12/13-49-50) (PLAB with PLAB prompt matrix)  
- [`14-16-51`](multirun/2026-02-12/14-16-51) (PubHealthBench with official prompt)

Run settings were consistent: `temperature=0.7`, `top_p=0.9`, `max_new_tokens=512`.
Datasets covered: `pubhealthbench` (reviewed split), `m_arc` (test), `medbullets` (op5_test), `plab`.  
Prompt families covered: `mcq` (with `few_shot=true/false` and `answer_first=true/false` sweeps), `pubhealthbench`, and `plab`.  

---

## Timeline Context

- **MedGemma release:** July 2025  
- **PubHealthBench release:** May 2025  
- **M-ARC release:** February 2025  
- **MedBullets dataset release:** March 2024  
- **PLAB dataset date:** N/A
- **Best OOD candidates for this sprint:** `pubhealthbench` and `plab`

---

## 2) Datasets and Prompt Matrix

**Question:**  
What OOD datasets and prompts were evaluated in this sprint?

**Bottom line:**  
All four datasets were run, but **PubHealthBench** and **PLAB** are the strongest OOD candidates for our core memorization-vs-reasoning analysis.

**Setup:**  
- Datasets: `pubhealthbench`, `m_arc`, `medbullets`, `plab`  
- Prompts tested: `mcq`, `pubhealthbench`, `plab`  
- Prompt flags tested: `few_shot`, `answer_first`

**Key findings:**  
- `mcq` prompt was used as the common baseline across all datasets.
- Dataset-specific prompts were added where available:
  - `pubhealthbench` dataset with `pubhealthbench` prompt ([`14-16-51/0`](multirun/2026-02-12/14-16-51/0))
  - `plab` dataset with `plab` prompt matrix ([`13-49-50/*`](multirun/2026-02-12/13-49-50))
- The strongest OOD candidates for reasoning-vs-memorization interpretation in this sprint are:
  - `pubhealthbench` (official-style prompt available, high final accuracy)
  - `plab` (both generic and dataset-specific prompt comparisons available)
- `m_arc` and `medbullets` were successfully included but currently serve more as **stress-test OOD sets** due to lower observed accuracy in this run set.

**Caveats:**  
- Not every dataset has a dedicated prompt strategy yet (`m_arc` and `medbullets` use `mcq` only).

---

## 3) PubHealthBench Results (UK Gov Public Health Information)

**Question:**  
What evidence does PubHealthBench provide for memorization vs reasoning, and secondarily how much does prompt choice shift the observed performance?

**Bottom line:**  
On PubHealthBench reviewed, the dataset-specific `pubhealthbench` prompt is dramatically stronger than generic `mcq`: **0.8598 vs 0.2483-0.3768** accuracy (up to **+48.3 points** vs best MCQ run).

**Setup:**  
- Dataset/split: `pubhealthbench` / `reviewed`
- Requested samples per run: `760`
- Generic MCQ sweep runs: `09-23-21/{0,1,2,3}`
- Dataset-specific run: `14-16-51/0`
- Shared decoding setup: `temperature=0.7`, `top_p=0.9`, `max_new_tokens=512`

**Key findings:**  
- `mcq` prompt (PubHealthBench reviewed):
  - `09-23-21/0` (`few_shot=true`, `answer_first=false`): **0.3768**
  - `09-23-21/1` (`few_shot=true`, `answer_first=true`): **0.3570**
  - `09-23-21/3` (`few_shot=false`, `answer_first=true`): **0.2801**
  - `09-23-21/2` (`few_shot=false`, `answer_first=false`): **0.2483**
- `pubhealthbench` prompt:
  - `14-16-51/0`: **0.8598** (`correct=650`, `incorrect=106`, `parse_errors=4`)
- Delta:
  - vs best MCQ run (`0.3768`): **+0.4830** (+48.3 points)
  - vs weakest MCQ run (`0.2483`): **+0.6114** (+61.1 points)

**Interpretation:**  
Results show very strong prompt-format sensitivity on the same dataset.  
The model can perform well on this OOD benchmark, but only when prompt structure completely aligns with dataset.  

**Caveats:**  
- Runs were sampled (`temperature=0.7`, `top_p=0.9`), so exact numbers may shift between reruns.
- `pubhealthbench` prompt run has `parse_errors=4`, while MCQ runs have `parse_errors=0`.
- This section uses the reviewed subset only (`760`), not the full PubHealthBench test set.

---

## 4) M-ARC Results (Clinical MCQ)

**Question:**  
What evidence does M-ARC provide for memorization vs reasoning, and secondarily how much do MCQ prompt toggles change outcomes?

**Bottom line:**  
M-ARC is a hard OOD stress test in this sprint: performance remains low across all settings (**0.1786-0.2556**), and MCQ prompt toggles produce only modest movement (**+7.7 points** best-to-worst).

**Setup:**  
- Dataset/split: `m_arc` / `test`
- Requested samples per run: `100`
- Runs: `09-23-21/{4,5,6,7}`
- Prompt family: `mcq` only (`few_shot` x `answer_first` matrix)
- Shared decoding setup: `temperature=0.7`, `top_p=0.9`, `max_new_tokens=512`

**Key findings:**  
- `09-23-21/4` (`few_shot=true`, `answer_first=false`): **0.2556** (`23/90`)
- `09-23-21/5` (`few_shot=true`, `answer_first=true`): **0.2111** (`19/90`)
- `09-23-21/6` (`few_shot=false`, `answer_first=false`): **0.2195** (`18/82`)
- `09-23-21/7` (`few_shot=false`, `answer_first=true`): **0.1786** (`15/84`)
- Best-to-worst spread: **+0.0770** (+7.7 points)
- Output-format behavior:
  - `parse_errors=0` in all runs, but many predictions are either missing (`predicted=None`) or outside expected option labels (`A-E`), which reduces the number of scored examples per run.

**Interpretation:**  
For memorization-vs-reasoning, M-ARC currently contributes a negative-transfer data point: the model does not show strong robust performance on this set under generic MCQ prompting.  
Since prompt toggles only move accuracy moderately and absolute scores stay low, current evidence here favors “limited task transfer” rather than strong benchmark-specific memorization.

**Caveats:**  
- No M-ARC-specific prompt strategy was tested in this batch (MCQ only).
- `num_samples=100` per run but only `82-90` were scored due missing/invalid predicted labels.

---

## 5) MedBullets Results (OOD Clinical MCQ)

**Question:**  
What evidence does MedBullets provide for memorization vs reasoning, and secondarily how much do MCQ prompt toggles change outcomes?

**Bottom line:**  
MedBullets is another difficult OOD set under generic MCQ prompting: accuracy remains low-to-moderate (**0.1760-0.2766**) and is highly affected by answer-format issues, with only **267-283** scored out of **308** samples per run.

**Setup:**  
- Dataset/split: `medbullets` / `op5_test`
- Requested samples per run: `308`
- Runs: `09-23-21/{8,9,10,11}`
- Prompt family: `mcq` only (`few_shot` x `answer_first` matrix)
- Shared decoding setup: `temperature=0.7`, `top_p=0.9`, `max_new_tokens=512`

**Key findings:**  
- `09-23-21/8` (`few_shot=true`, `answer_first=false`): **0.2766** (`78/282`)
- `09-23-21/9` (`few_shot=true`, `answer_first=true`): **0.2650** (`75/283`)
- `09-23-21/10` (`few_shot=false`, `answer_first=false`): **0.2117** (`58/274`)
- `09-23-21/11` (`few_shot=false`, `answer_first=true`): **0.1760** (`47/267`)
- Best-to-worst spread: **+0.1006** (+10.1 points)
- Output-format behavior:
  - `parse_errors=0` in all runs, but many predictions are missing (`None`) or outside `A-E`, reducing the scored pool.

**Interpretation:**  
For memorization-vs-reasoning, MedBullets adds another low-transfer signal under the current generic MCQ setup.  

**Caveats:**  
- This sprint section uses `op5_test` only; `op4_test` was not part of this multirun batch.

---

## 6) PLAB Results (UK Licensing Exam)

**Question:**  
What evidence does PLAB provide for memorization vs reasoning, and secondarily does a PLAB-specific prompt materially improve results over MCQ?

**Bottom line:**  
PLAB is the strongest positive OOD signal in this sprint: with matched prompt format, performance rises substantially above generic MCQ on the full PLAB set. Using reliable full-dataset runs, PLAB prompt is around **0.691-0.695** vs MCQ around **0.226-0.410**.

**Setup:**  
- Full-dataset comparison (`num_samples=1652`, `split=main`, model `google/medgemma-27b_text-it`):
  - MCQ matrix: `12-33-38/{0,1,2,3}`
  - PLAB prompt matrix: `13-49-50/{0,1,2,3}`
- Small ablation runs (model `google/medgemma-4b-it`, `num_samples=100`):
  - `pubhealthbench` prompt on PLAB: `outputs/2026-02-12/07-43-39...`
  - companion `mcq`/`plab` prompt runs on the same 100-sample subset

**Key findings:**  
- `mcq` prompt (full PLAB, 27B):
  - `12-33-38/0` (`few_shot=true`, `answer_first=false`): **0.5202** (`733/1409`)
  - `12-33-38/1` (`few_shot=true`, `answer_first=true`): **0.4096** (`648/1582`)
  - `12-33-38/2` (`few_shot=false`, `answer_first=false`): **0.2325** (`342/1471`)
  - `12-33-38/3` (`few_shot=false`, `answer_first=true`): **0.2257** (`328/1453`)
- `plab` prompt (full PLAB, 27B):
  - `13-49-50/0` (`few_shot=true`, `answer_first=false`): **0.7457** (`730/979`) but **673 unscored outputs** (anomalous)
  - `13-49-50/1` (`few_shot=false`, `answer_first=false`): **0.6913** (`1140/1649`)
  - `13-49-50/2` (`few_shot=true`, `answer_first=true`): **0.6952** (`1106/1591`)
  - `13-49-50/3` (`few_shot=false`, `answer_first=true`): **0.6930** (`1142/1648`)
- `pubhealthbench` prompt on PLAB (4B, 100-sample ablation):
  - `07-43-39`: **0.5400** (`54/100`)
  - same subset reference points: `mcq` up to **0.65**, `plab` around **0.55-0.62**
- Delta (full, reliable runs):
  - best reliable PLAB-prompt run (`0.6952`) vs best stable MCQ run (`0.4096`): **+0.2856** (+28.6 points)

**Interpretation:**  
PLAB supports a mixed but strong transfer story: the model can perform much better on this OOD exam when prompt framing matches dataset conventions.  
This is not consistent with pure memorization-only behavior; however, it also shows that usable reasoning signal is strongly gated by prompt and answer-format alignment.

**Caveats:**  
- Reported `accuracy` is computed on scored predictions (`correct + incorrect`), so runs with many unscored responses can look artificially high.

---

## 7) Prompt Sensitivity vs Reasoning Signal

**Question:**  
Across OOD datasets, what do prompt-sensitivity patterns tell us about memorization vs reasoning, and secondarily how much does each prompt control change outcomes?

**Bottom line:**  
Prompt sensitivity is a first-order factor in this sprint. `few_shot` generally helps MCQ performance, `answer_first` is mostly neutral-to-negative, and the largest gains come from dataset-specific prompts. This supports “conditional reasoning transfer” rather than prompt-invariant robust reasoning.

**Setup:**  
- MCQ matrix comparisons:
  - `09-23-21` for `pubhealthbench`, `m_arc`, `medbullets`
  - `12-33-38` for `plab`
- Dataset-specific prompt comparisons:
  - `14-16-51/0` (`pubhealthbench` prompt on PubHealthBench)
  - `13-49-50/{1,2,3}` as reliable PLAB-prompt runs on PLAB
- Same decoding family for sprint runs: `temperature=0.7`, `top_p=0.9`, `max_new_tokens=512`

**Key findings:**  
- Effect of dataset-specific prompts (largest shifts):
  - PubHealthBench prompt vs best MCQ on PubHealthBench: **+0.4830** (+48.3 points)
  - PLAB prompt vs best stable MCQ on PLAB: **+0.2856** (+28.6 points)
  - `m_arc` and `medbullets` currently lack dedicated prompt strategies in this sprint.

**Interpretation:**  
The model’s OOD performance is strongly controlled by prompt/task framing, which argues against a simple “memorized vs not memorized” binary.  
Large gains from dataset-specific prompts indicate substantial latent capability that generic prompting fails to consistently unlock.  
At the same time, weak generic-MCQ transfer on M-ARC and MedBullets shows that reasoning is not robustly portable across all OOD formats.

**Caveats:**  
- Accuracy is computed on scored predictions; unscored outputs vary across runs and can distort direct comparisons.
- Stochastic decoding (`temperature=0.7`) adds run-to-run variability.
- No dataset-specific prompts were tested for `m_arc`/`medbullets`, so prompt sensitivity is likely underestimated there.

---

## 8) Answer to Sprint Question

**Question:**  
Based on current OOD evidence, is the model memorizing or reasoning?

**Bottom line:**  
Current evidence supports **conditional reasoning transfer**, not pure memorization and not robust prompt-invariant reasoning. As a result, **prompt-dataset alignment is more important than whether the dataset is OOD or not**.

**Evidence summary:**  
- The model shows large gains when prompt format matches dataset conventions:
  - PubHealthBench: `pubhealthbench` prompt `0.8598` vs best MCQ `0.3768` (**+48.3 points**).
  - PLAB: reliable `plab` prompt `~0.691-0.695` vs stable MCQ `0.4096` (**+28.6 points** at best).
- Generic MCQ transfer remains weak on some OOD sets:
  - M-ARC: `0.1786-0.2556`
  - MedBullets: `0.1760-0.2766`
- Prompt controls matter:
  - `few_shot` generally improves results.
  - `answer_first` is mostly neutral-to-negative outside limited cases.

---

## References

- https://huggingface.co/datasets/Joshua-Harris/PubHealthBench
- https://huggingface.co/datasets/mkieffer/Medbullets
- https://huggingface.co/datasets/mkieffer/M-ARC
