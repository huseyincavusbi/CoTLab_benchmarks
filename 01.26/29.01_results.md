# Findings Summary

## 1) Think‑first vs Answer-first (MCQ)

**Question:**
Can we assess how relevant “think‑first” ordering is to performance? (answer_first=true vs false)

**Bottom line:**
Think‑first (answer_first=false) improves MCQ accuracy by ~5–15 points on most datasets for both 4B and 27B.

**Setup:**
Full‑dataset MCQ runs (`prompt=mcq`, `few_shot=true`, `output_format=json`) on MedGemma‑4B and MedGemma‑27B‑text‑it, plus full‑dataset PubMedQA runs using the PubMedQA prompt.

**Key findings:**
- **"Answer‑first=false" outperforms "true" on most datasets** for both models.
- **MedGemma‑4B:** switching to think‑first improves accuracy on **5 of 6 datasets**. The largest gains are **PubMedQA (+15.2 points)** and **MMLU‑medical (+9.7 points)**; **MedXpertQA is essentially unchanged** (‑0.2).
- **MedGemma‑27B:** think‑first helps on **4 of 6 datasets**, with the biggest jump on **MedQA (+13.7 points)**. Two datasets show small reversals (**PubMedQA ‑2.2**, **AfriMedQA ‑0.7**), suggesting minor variation rather than a strong negative trend.

**Interpretation:**
“Think‑first” (answer_first=false) is generally better for MCQ performance; answer‑first appears to suppress accuracy except in a few small reversals.

**Caveats:**
- Runs are few‑shot and JSON‑structured, unlike official zero‑shot evals.
- Decoding settings (temperature/top_p) differ from official benchmarks.

---

## 2) Why is Histo performance low?

**Question:**
Why is pipeline performance low on Histopathology?

**Bottom line:**
Histo is a harder, more ambiguous, comparative task, so lower accuracy is expected even for strong models.

**Setup:**
Histo evaluated with text‑only prompts; analysis based on histo attention results + task characteristics.

**Key findings:**
- Histo is **not a simple diagnosis task**; it is a **comparative grading** task (0/1/2) against a reference case.
- Labels are **more subjective / ambiguous** than radiology/oncology.
- The task requires **alignment between two texts** (report vs source case), not just label extraction.

**Interpretation:**
These structural differences alone can cap accuracy and explain why histo underperforms compared to simpler binary tasks.

---

## 3) Internal circuits — is the model “thinking differently” for Histo?

**Question:**
Can we check the internal circuit activations to see if the LLM is thinking differently for Histo vs other datasets?

**Bottom line:**
Yes. Attention patterns differ by dataset, and causal patching shows late‑layer circuits (29–33) drive dataset‑specific behavior.

**Setup:**
AttentionAnalysis runs: MedGemma‑4B on Histo (100 samples, all layers) and MedGemma‑27B‑text‑it on Histo/Radiology/MedQA (20 samples each). Activation patching (100‑sample cross‑dataset) + head‑level sweeps (5 & 50 samples).
Run IDs:
- Attention analysis (4B Histo): [2026-01-28/18-38-48_attention_analysis_medgemma_4b_histopathology_json_histopathology](2026-01-28/18-38-48_attention_analysis_medgemma_4b_histopathology_json_histopathology)
- Attention analysis (27B Histo/Radiology/MedQA): [2026-01-27/20-55-10_attention_analysis_medgemma_27b_text_it_histopathology_json_histopathology](2026-01-27/20-55-10_attention_analysis_medgemma_27b_text_it_histopathology_json_histopathology), [2026-01-27/20-57-09_attention_analysis_medgemma_27b_text_it_radiology_json_radiology](2026-01-27/20-57-09_attention_analysis_medgemma_27b_text_it_radiology_json_radiology), [2026-01-27/20-55-52_attention_analysis_medgemma_27b_text_it_mcq_json_medqa](2026-01-27/20-55-52_attention_analysis_medgemma_27b_text_it_mcq_json_medqa)
- Activation patching (100‑sample, cross‑dataset): [2026-01-28/17-35-31_activation_patching_medgemma_4b_clean_json_corrupt](2026-01-28/17-35-31_activation_patching_medgemma_4b_clean_json_corrupt), [2026-01-28/17-51-24_activation_patching_medgemma_4b_clean_onco_json_corrupt_neuro](2026-01-28/17-51-24_activation_patching_medgemma_4b_clean_onco_json_corrupt_neuro), [2026-01-28/18-03-51_activation_patching_medgemma_4b_clean_cardio_json_corrupt_pubmed](2026-01-28/18-03-51_activation_patching_medgemma_4b_clean_cardio_json_corrupt_pubmed), [2026-01-28/18-25-14_activation_patching_medgemma_4b_clean_mmlu_json_corrupt_onco](2026-01-28/18-25-14_activation_patching_medgemma_4b_clean_mmlu_json_corrupt_onco)
- Head‑level sweeps: [2026-01-28/18-47-18_activation_patching_medgemma_4b_clean_json_corrupt](2026-01-28/18-47-18_activation_patching_medgemma_4b_clean_json_corrupt) (5 samples), [2026-01-28/18-55-36_activation_patching_medgemma_4b_clean_json_corrupt](2026-01-28/18-55-36_activation_patching_medgemma_4b_clean_json_corrupt) (50 samples)

**Key findings:**
- **Attention analysis (entropy, 27B, 20 samples):** Histo **2.1257** (most focused layer **3**, **0.2532**), Radiology **1.9476** (layer **3**, **0.1960**), MedQA **2.2271** (layer **3**, **0.5482**).
- **Attention analysis (entropy, 4B Histo only, 100 samples):** overall **2.7192** (most focused layer **32**, **1.4914**).
- **Layer‑level activation patching:** late layers **29–33 consistently dominate** across multiple dataset pairs.
- **Head‑level sweeps:** top heads cluster in **layer 30** (e.g., 30:6, 30:4, 30:3, 30:2).

**Interpretation:**
We now have evidence that internal circuits differ, and the **differences concentrate in late layers**. Attention patterns differ across datasets (e.g., Histo vs Radiology in 27B), while patching shows late‑layer causal dominance.

**Caveats:**
- Head sweeps used **5 and 50 samples**; stability across seeds not yet tested.

---

## 4) Is Histo fundamentally different from other datasets?

**Question:**
Is Histo fundamentally different from other medical datasets?

**Bottom line:**
Histo is behaviorally and structurally distinct, which likely explains its lower performance and different processing patterns.

**Setup:**
Comparison across Histo, Radiology, MedQA via attention behavior + task structure analysis.

**Key findings:**
- In 27B attention runs, Histo shows **higher entropy than Radiology** (more diffuse attention), while MedQA is higher still.
- In the 4B Histo attention run, the **most focused layer is late (32)**, but there is no 4B cross‑dataset attention baseline yet.
- Task structure is **comparative grading**, not direct diagnosis.
- Labels are **more ambiguous**.

**Caveats:**
- Not yet controlled for prompt structure differences.

---

## 5) Official MedGemma benchmarks vs our results

**Question:**
What official benchmarks/prompts does Google use, and how do they compare to our setup?

**Bottom line:**
Our results are not directly comparable to official benchmarks because prompts and decoding differ; matching them is required for a fair comparison.

**Setup:**
Comparison of official MedGemma text‑only benchmark results vs our full‑run MCQ results.

**Key findings:**
- Official accuracies are substantially higher (e.g., MedQA 27B = **89.8% best‑of‑5, 87.7% 0‑shot**; our runs are lower).
- **Prompting differs substantially:** official = **zero‑shot**, short, “Final Answer:(X)”, **temp=0**; ours = **few‑shot**, long, JSON, system role, non‑zero temp.
- Official 27B scores use **test‑time scaling** (best‑of‑5), while ours are single‑pass.

**Interpretation:**
The gap is expected. The setups are not comparable unless prompts and decoding are aligned. Without matching prompts + decoding, our results should not be directly compared to official benchmarks.

**Caveats:**
- Different output formats and decoding settings can shift accuracy significantly.
